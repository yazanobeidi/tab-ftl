(gb) yazan:gb % python -W ignore::UserWarning run.py --device cpu --epochs 105 --model model --batches 10 --seed-range 8:28                         
Running both --federate and --no-federate as comparison.
Running both --personalize and --no-personalize as comparison (for --federate runs only)
Running for seeds [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]...
Loading personalizedfederated trainer for seed 8...(1/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_8//104638/20230409_personalizedfederated_cover_split_by_wilderness_model_8/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.994 valid 0.830 comanche:auroc: train 0.991 valid 0.946 comanche:auprc: train 0.946 valid 0.809 comanche:recall: train 0.994 valid 0.835 comanche:precision: train 0.994 valid 0.829 comanche:accuracy: train 0.994 valid 0.816 comanche:loss: train 0.017 valid 1.111 neota:f1_score: train 1.000 valid 0.889 neota:auroc: train 1.000 valid 0.924 neota:auprc: train 1.000 valid 0.839 neota:recall: train 1.000 valid 0.894 neota:precision: train 1.000 valid 0.888 neota:accuracy: train 1.000 valid 0.808 neota:loss: train 0.002 valid 0.844 poudre:f1_score: train 0.999 valid 0.909 poudre:auroc: train 0.999 valid 0.950 poudre:auprc: train 0.986 valid 0.752 poudre:recall: train 0.999 valid 0.911 poudre:precision: train 0.999 valid 0.912 poudre:accuracy: train 0.999 valid 0.719 poudre:loss: train 0.002 valid 0.583 rawah:f1_score: train 1.000 valid 0.835 rawah:auroc: train 0.999 valid 0.945 rawah:auprc: train 0.997 valid 0.880 rawah:recall: train 1.000 valid 0.837 rawah:precision: train 1.000 valid 0.836 rawah:accuracy: train 1.000 valid 0.861 rawah:loss: train 0.002 valid 1.624 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.70499
auroc 0.92312
auprc 0.68225
recall 0.69139
precision 0.75884
confusion_matrix
    0     1     2     3     4     5     6
    1 63118 13710   270  2424   615  6474
    2 22266 74305  5031 13584  6990  1977
    3    19   150 11882   338  1014    34
    4    46   154    88  4081    36     3
    5    28    97   493   183  5863     0
    6   166    34     0     8     0 11534
accuracy 0.83324
loss 2.32526

neota:
f1_score 0.81816
auroc 0.91996
auprc 0.88145
recall 0.82035
precision 0.82502
confusion_matrix
    0     1     2     3
    1 16118  1443   853
    2  2752  5943   224
    3     7     0  2045
accuracy 0.84608
loss 1.39869

poudre:
f1_score 0.80019
auroc 0.91210
auprc 0.73753
recall 0.80423
precision 0.84809
confusion_matrix
    0     1     2     3     4
    1   685  1499    55   767
    2    37 17012  1327  1781
    3     0    12   567     8
    4    14   498   324  7707
accuracy 0.73498
loss 1.57800

rawah:
f1_score 0.77485
auroc 0.91910
auprc 0.75104
recall 0.76470
precision 0.79622
confusion_matrix
     0      1      2      3      4
     1  77587  19445   1243   6380
     2  25092 112017   7213    741
     3     12     97   2816      0
     4    252     44      0   4260
accuracy 0.85283
loss 2.09431

saving test results to cover/log/89/seed_8//104638/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 9...(2/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_9//104952/20230409_personalizedfederated_cover_split_by_wilderness_model_9/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.984 valid 0.824 comanche:auroc: train 0.975 valid 0.931 comanche:auprc: train 0.848 valid 0.732 comanche:recall: train 0.984 valid 0.828 comanche:precision: train 0.984 valid 0.824 comanche:accuracy: train 0.983 valid 0.811 comanche:loss: train 0.048 valid 1.120 neota:f1_score: train 0.997 valid 0.871 neota:auroc: train 0.999 valid 0.952 neota:auprc: train 0.989 valid 0.877 neota:recall: train 0.997 valid 0.876 neota:precision: train 0.997 valid 0.868 neota:accuracy: train 0.994 valid 0.776 neota:loss: train 0.005 valid 0.720 poudre:f1_score: train 0.995 valid 0.909 poudre:auroc: train 0.999 valid 0.897 poudre:auprc: train 0.993 valid 0.771 poudre:recall: train 0.995 valid 0.912 poudre:precision: train 0.995 valid 0.910 poudre:accuracy: train 0.996 valid 0.677 poudre:loss: train 0.019 valid 0.452 rawah:f1_score: train 0.998 valid 0.819 rawah:auroc: train 0.998 valid 0.944 rawah:auprc: train 0.991 valid 0.875 rawah:recall: train 0.998 valid 0.821 rawah:precision: train 0.998 valid 0.820 rawah:accuracy: train 0.998 valid 0.844 rawah:loss: train 0.008 valid 1.215 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.71270
auroc 0.91918
auprc 0.65497
recall 0.69984
precision 0.76228
confusion_matrix
    0     1     2     3     4     5     6
    1 64793 13638   208  2546   637  4789
    2 23518 75023  4263 13125  6929  1295
    3    13   220 11641   375  1182     6
    4    43   148    90  4085    42     0
    5     8   166   443   124  5923     0
    6   288    43     0     5     0 11406
accuracy 0.83427
loss 2.10157

neota:
f1_score 0.76882
auroc 0.89219
auprc 0.85869
recall 0.76961
precision 0.77989
confusion_matrix
    0     1     2     3
    1 15107  2051  1256
    2  3082  5457   380
    3     0     1  2051
accuracy 0.81059
loss 1.54922

poudre:
f1_score 0.79640
auroc 0.87979
auprc 0.72292
recall 0.80451
precision 0.84016
confusion_matrix
    0     1     2     3     4
    1   569  1613   150   674
    2    62 17184  1057  1854
    3     0     7   574     6
    4    24   498   368  7653
accuracy 0.72887
loss 1.85710

rawah:
f1_score 0.77473
auroc 0.91336
auprc 0.67892
recall 0.76467
precision 0.79601
confusion_matrix
     0      1      2      3      4
     1  77622  19451   1424   6158
     2  25150 111983   6884   1046
     3     16    101   2807      1
     4    258     36      2   4260
accuracy 0.85209
loss 1.62967

saving test results to cover/log/89/seed_9//104952/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 10...(3/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_10//105313/20230409_personalizedfederated_cover_split_by_wilderness_model_10/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.988 valid 0.830 comanche:auroc: train 0.984 valid 0.944 comanche:auprc: train 0.897 valid 0.761 comanche:recall: train 0.988 valid 0.832 comanche:precision: train 0.988 valid 0.829 comanche:accuracy: train 0.988 valid 0.815 comanche:loss: train 0.044 valid 0.980 neota:f1_score: train 0.989 valid 0.857 neota:auroc: train 0.999 valid 0.909 neota:auprc: train 0.996 valid 0.815 neota:recall: train 0.989 valid 0.858 neota:precision: train 0.990 valid 0.856 neota:accuracy: train 0.974 valid 0.778 neota:loss: train 0.029 valid 0.718 poudre:f1_score: train 0.988 valid 0.910 poudre:auroc: train 0.997 valid 0.942 poudre:auprc: train 0.974 valid 0.785 poudre:recall: train 0.988 valid 0.912 poudre:precision: train 0.988 valid 0.912 poudre:accuracy: train 0.990 valid 0.719 poudre:loss: train 0.037 valid 0.519 rawah:f1_score: train 0.997 valid 0.823 rawah:auroc: train 0.996 valid 0.942 rawah:auprc: train 0.985 valid 0.856 rawah:recall: train 0.997 valid 0.824 rawah:precision: train 0.997 valid 0.825 rawah:accuracy: train 0.998 valid 0.845 rawah:loss: train 0.010 valid 1.131 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.72339
auroc 0.92607
auprc 0.65891
recall 0.71396
precision 0.75969
confusion_matrix
    0     1     2     3     4     5     6
    1 62138 16448   332  1805   394  5494
    2 20970 80961  4305  9282  7322  1313
    3    21   244 11717   329  1122     4
    4    31   218    80  4024    55     0
    5    22   114   415   120  5992     1
    6   180    34     0     1     0 11527
accuracy 0.83921
loss 1.95863

neota:
f1_score 0.78812
auroc 0.89581
auprc 0.83784
recall 0.79241
precision 0.79558
confusion_matrix
    0     1     2     3
    1 15864  1629   921
    2  3330  5378   211
    3     9     0  2043
accuracy 0.82004
loss 0.89959

poudre:
f1_score 0.79210
auroc 0.86638
auprc 0.69380
recall 0.79692
precision 0.84403
confusion_matrix
    0     1     2     3     4
    1   612  1557    47   790
    2    29 16978  1394  1756
    3     0     7   573     7
    4     4   605   362  7572
accuracy 0.72709
loss 1.82190

rawah:
f1_score 0.76695
auroc 0.90760
auprc 0.61333
recall 0.75339
precision 0.79504
confusion_matrix
     0      1      2      3      4
     1  77901  17665   1469   7620
     2  26732 108839   8285   1207
     3     16    116   2792      1
     4    284     33      0   4239
accuracy 0.84490
loss 1.59009

saving test results to cover/log/89/seed_10//105313/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 11...(4/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_11//105635/20230409_personalizedfederated_cover_split_by_wilderness_model_11/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.975 valid 0.834 comanche:auroc: train 0.989 valid 0.944 comanche:auprc: train 0.942 valid 0.800 comanche:recall: train 0.975 valid 0.838 comanche:precision: train 0.975 valid 0.833 comanche:accuracy: train 0.973 valid 0.819 comanche:loss: train 0.072 valid 1.008 neota:f1_score: train 0.997 valid 0.912 neota:auroc: train 1.000 valid 0.955 neota:auprc: train 1.000 valid 0.843 neota:recall: train 0.997 valid 0.912 neota:precision: train 0.997 valid 0.915 neota:accuracy: train 0.994 valid 0.872 neota:loss: train 0.006 valid 0.817 poudre:f1_score: train 0.990 valid 0.913 poudre:auroc: train 0.992 valid 0.913 poudre:auprc: train 0.957 valid 0.776 poudre:recall: train 0.990 valid 0.914 poudre:precision: train 0.990 valid 0.912 poudre:accuracy: train 0.973 valid 0.722 poudre:loss: train 0.029 valid 0.487 rawah:f1_score: train 1.000 valid 0.833 rawah:auroc: train 0.998 valid 0.945 rawah:auprc: train 0.995 valid 0.882 rawah:recall: train 1.000 valid 0.835 rawah:precision: train 1.000 valid 0.835 rawah:accuracy: train 1.000 valid 0.857 rawah:loss: train 0.001 valid 1.235 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.72025
auroc 0.92715
auprc 0.67075
recall 0.70633
precision 0.76872
confusion_matrix
    0     1     2     3     4     5     6
    1 63686 13688   154  2451   417  6215
    2 21577 77523  3846 13358  6546  1303
    3    57   194 11850   360   966    10
    4    25   128    83  4123    49     0
    5    15   120   565   170  5783    11
    6   204    24     0     5     0 11509
accuracy 0.83749
loss 1.74719

neota:
f1_score 0.79734
auroc 0.91016
auprc 0.86185
recall 0.79643
precision 0.80765
confusion_matrix
    0     1     2     3
    1 15298  1963  1153
    2  2512  6057   350
    3     4     0  2048
accuracy 0.83598
loss 1.39640

poudre:
f1_score 0.80867
auroc 0.87141
auprc 0.71231
recall 0.81460
precision 0.84800
confusion_matrix
    0     1     2     3     4
    1   752  1582    61   611
    2    46 17427  1102  1582
    3     0    11   568     8
    4    11   707   266  7559
accuracy 0.74179
loss 1.73086

rawah:
f1_score 0.77696
auroc 0.91469
auprc 0.71698
recall 0.76618
precision 0.79912
confusion_matrix
     0      1      2      3      4
     1  76815  19733   1183   6924
     2  23640 113176   7332    915
     3      7    142   2776      0
     4    237     26      0   4293
accuracy 0.85138
loss 1.78105

saving test results to cover/log/89/seed_11//105635/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 12...(5/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_12//105953/20230409_personalizedfederated_cover_split_by_wilderness_model_12/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.994 valid 0.841 comanche:auroc: train 0.989 valid 0.944 comanche:auprc: train 0.908 valid 0.760 comanche:recall: train 0.994 valid 0.842 comanche:precision: train 0.994 valid 0.841 comanche:accuracy: train 0.994 valid 0.828 comanche:loss: train 0.020 valid 0.932 neota:f1_score: train 0.997 valid 0.873 neota:auroc: train 0.996 valid 0.914 neota:auprc: train 0.985 valid 0.814 neota:recall: train 0.997 valid 0.876 neota:precision: train 0.997 valid 0.871 neota:accuracy: train 0.994 valid 0.792 neota:loss: train 0.004 valid 1.183 poudre:f1_score: train 0.996 valid 0.915 poudre:auroc: train 0.995 valid 0.962 poudre:auprc: train 0.919 valid 0.745 poudre:recall: train 0.996 valid 0.917 poudre:precision: train 0.996 valid 0.915 poudre:accuracy: train 0.997 valid 0.721 poudre:loss: train 0.012 valid 0.511 rawah:f1_score: train 1.000 valid 0.851 rawah:auroc: train 0.999 valid 0.950 rawah:auprc: train 0.996 valid 0.896 rawah:recall: train 1.000 valid 0.852 rawah:precision: train 1.000 valid 0.851 rawah:accuracy: train 1.000 valid 0.874 rawah:loss: train 0.001 valid 1.063 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.73719
auroc 0.92989
auprc 0.67214
recall 0.72415
precision 0.77808
confusion_matrix
    0     1     2     3     4     5     6
    1 64604 14761   284  2576   444  3942
    2 19647 80949  3921 11976  6815   845
    3    28   193 11834   318  1059     5
    4    24   158    88  4103    35     0
    5     6    90   442   125  6001     0
    6   307    42     2     5     0 11386
accuracy 0.84660
loss 1.83395

neota:
f1_score 0.79396
auroc 0.90264
auprc 0.75505
recall 0.79377
precision 0.80277
confusion_matrix
    0     1     2     3
    1 15321  2033  1060
    2  2616  5959   344
    3     3     4  2045
accuracy 0.83225
loss 2.10983

poudre:
f1_score 0.81277
auroc 0.91413
auprc 0.69718
recall 0.81448
precision 0.84838
confusion_matrix
    0     1     2     3     4
    1   888  1484    65   569
    2   107 17193  1179  1678
    3     0     5   573     9
    4    14   576   305  7648
accuracy 0.75494
loss 1.55624

rawah:
f1_score 0.78149
auroc 0.92003
auprc 0.72894
recall 0.77140
precision 0.80292
confusion_matrix
     0      1      2      3      4
     1  79114  18278   1195   6068
     2  24860 112224   7144    835
     3      8    103   2814      0
     4    271     32      1   4252
accuracy 0.85622
loss 1.70122

saving test results to cover/log/89/seed_12//105953/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 13...(6/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_13//110312/20230409_personalizedfederated_cover_split_by_wilderness_model_13/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.998 valid 0.841 comanche:auroc: train 0.993 valid 0.947 comanche:auprc: train 0.938 valid 0.790 comanche:recall: train 0.998 valid 0.845 comanche:precision: train 0.998 valid 0.840 comanche:accuracy: train 0.998 valid 0.828 comanche:loss: train 0.006 valid 1.281 neota:f1_score: train 0.997 valid 0.855 neota:auroc: train 1.000 valid 0.913 neota:auprc: train 0.999 valid 0.817 neota:recall: train 0.997 valid 0.858 neota:precision: train 0.997 valid 0.852 neota:accuracy: train 0.993 valid 0.760 neota:loss: train 0.007 valid 1.226 poudre:f1_score: train 0.980 valid 0.896 poudre:auroc: train 0.997 valid 0.866 poudre:auprc: train 0.989 valid 0.742 poudre:recall: train 0.980 valid 0.898 poudre:precision: train 0.980 valid 0.898 poudre:accuracy: train 0.965 valid 0.708 poudre:loss: train 0.063 valid 0.454 rawah:f1_score: train 0.986 valid 0.842 rawah:auroc: train 0.996 valid 0.952 rawah:auprc: train 0.991 valid 0.896 rawah:recall: train 0.986 valid 0.844 rawah:precision: train 0.986 valid 0.841 rawah:accuracy: train 0.988 valid 0.863 rawah:loss: train 0.043 valid 0.808 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.72357
auroc 0.92563
auprc 0.66596
recall 0.71204
precision 0.76852
confusion_matrix
    0     1     2     3     4     5     6
    1 64867 13707   183  2221   384  5249
    2 22318 77681  4257 11799  6677  1421
    3    25   214 11812   410   968     8
    4    32   144    74  4123    35     0
    5    18   113   472   139  5921     1
    6   219    38     0     3     1 11481
accuracy 0.84255
loss 2.34412

neota:
f1_score 0.74317
auroc 0.87979
auprc 0.81818
recall 0.74786
precision 0.75039
confusion_matrix
    0     1     2     3
    1 15065  2255  1094
    2  3756  4864   299
    3     4     1  2047
accuracy 0.78701
loss 1.76798

poudre:
f1_score 0.80434
auroc 0.88584
auprc 0.72197
recall 0.80739
precision 0.84170
confusion_matrix
    0     1     2     3     4
    1   795  1549    96   566
    2    65 17241  1186  1665
    3     0     6   575     6
    4    42   724   315  7462
accuracy 0.74321
loss 1.30264

rawah:
f1_score 0.77856
auroc 0.91840
auprc 0.70260
recall 0.76808
precision 0.80070
confusion_matrix
     0      1      2      3      4
     1  79116  18233   1356   5950
     2  25516 111370   7604    573
     3     13    108   2804      0
     4    242     53      2   4259
accuracy 0.85429
loss 6.43481

saving test results to cover/log/89/seed_13//110312/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 14...(7/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_14//110635/20230409_personalizedfederated_cover_split_by_wilderness_model_14/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.996 valid 0.842 comanche:auroc: train 0.990 valid 0.947 comanche:auprc: train 0.918 valid 0.787 comanche:recall: train 0.996 valid 0.845 comanche:precision: train 0.996 valid 0.841 comanche:accuracy: train 0.995 valid 0.829 comanche:loss: train 0.018 valid 1.043 neota:f1_score: train 0.997 valid 0.859 neota:auroc: train 1.000 valid 0.944 neota:auprc: train 1.000 valid 0.837 neota:recall: train 0.997 valid 0.867 neota:precision: train 0.997 valid 0.855 neota:accuracy: train 0.994 valid 0.753 neota:loss: train 0.005 valid 0.885 poudre:f1_score: train 0.995 valid 0.911 poudre:auroc: train 0.985 valid 0.946 poudre:auprc: train 0.961 valid 0.802 poudre:recall: train 0.995 valid 0.912 poudre:precision: train 0.995 valid 0.911 poudre:accuracy: train 0.961 valid 0.761 poudre:loss: train 0.024 valid 0.471 rawah:f1_score: train 0.999 valid 0.830 rawah:auroc: train 0.998 valid 0.947 rawah:auprc: train 0.995 valid 0.890 rawah:recall: train 0.999 valid 0.832 rawah:precision: train 0.999 valid 0.831 rawah:accuracy: train 0.999 valid 0.856 rawah:loss: train 0.006 valid 1.007 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.71957
auroc 0.92550
auprc 0.68744
recall 0.70897
precision 0.76090
confusion_matrix
    0     1     2     3     4     5     6
    1 63969 14905   153  2112   378  5094
    2 23200 77856  4262 11794  5951  1090
    3    53   209 11810   365   998     2
    4    47   155    80  4089    37     0
    5    15   141   465   112  5928     3
    6   235    27     0     6     0 11474
accuracy 0.83983
loss 1.94618

neota:
f1_score 0.76478
auroc 0.88713
auprc 0.83905
recall 0.76890
precision 0.77858
confusion_matrix
    0     1     2     3
    1 15586  1618  1210
    2  3507  4976   436
    3    13     7  2032
accuracy 0.79819
loss 1.50674

poudre:
f1_score 0.80659
auroc 0.90616
auprc 0.74370
recall 0.81182
precision 0.84847
confusion_matrix
    0     1     2     3     4
    1   737  1578    73   618
    2    40 17400  1128  1589
    3     0     9   573     5
    4     7   700   330  7506
accuracy 0.74079
loss 1.28331

rawah:
f1_score 0.77633
auroc 0.91784
auprc 0.68805
recall 0.76492
precision 0.80016
confusion_matrix
     0      1      2      3      4
     1  78745  18149   1262   6499
     2  25375 110950   7781    957
     3     12    104   2809      0
     4    292     32      0   4232
accuracy 0.85162
loss 1.47536

saving test results to cover/log/89/seed_14//110635/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 15...(8/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_15//111000/20230409_personalizedfederated_cover_split_by_wilderness_model_15/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.977 valid 0.826 comanche:auroc: train 0.977 valid 0.941 comanche:auprc: train 0.877 valid 0.774 comanche:recall: train 0.977 valid 0.828 comanche:precision: train 0.977 valid 0.825 comanche:accuracy: train 0.976 valid 0.810 comanche:loss: train 0.072 valid 0.949 neota:f1_score: train 0.997 valid 0.889 neota:auroc: train 0.999 valid 0.928 neota:auprc: train 0.999 valid 0.822 neota:recall: train 0.997 valid 0.894 neota:precision: train 0.997 valid 0.888 neota:accuracy: train 0.994 valid 0.808 neota:loss: train 0.006 valid 1.104 poudre:f1_score: train 0.999 valid 0.910 poudre:auroc: train 1.000 valid 0.864 poudre:auprc: train 0.997 valid 0.828 poudre:recall: train 0.999 valid 0.911 poudre:precision: train 0.999 valid 0.910 poudre:accuracy: train 0.999 valid 0.760 poudre:loss: train 0.002 valid 0.769 rawah:f1_score: train 1.000 valid 0.819 rawah:auroc: train 0.995 valid 0.936 rawah:auprc: train 0.987 valid 0.876 rawah:recall: train 1.000 valid 0.821 rawah:precision: train 1.000 valid 0.819 rawah:accuracy: train 1.000 valid 0.843 rawah:loss: train 0.002 valid 1.383 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.72994
auroc 0.91617
auprc 0.65571
recall 0.71816
precision 0.77507
confusion_matrix
    0     1     2     3     4     5     6
    1 66346 12867   121  2114   496  4667
    2 22554 77971  3780 11407  7316  1125
    3    14   271 11678   340  1129     5
    4    38   173   110  4024    63     0
    5    38   125   472   189  5839     1
    6   189    11     0     4     0 11538
accuracy 0.83914
loss 1.58256

neota:
f1_score 0.76248
auroc 0.88136
auprc 0.80480
recall 0.76910
precision 0.77421
confusion_matrix
    0     1     2     3
    1 15745  1620  1049
    2  3762  4806   351
    3     3     0  2049
accuracy 0.79748
loss 1.67508

poudre:
f1_score 0.80637
auroc 0.87728
auprc 0.70959
recall 0.80674
precision 0.84665
confusion_matrix
    0     1     2     3     4
    1   883  1557    72   494
    2    41 17101  1285  1730
    3     0    11   568     8
    4    27   676   340  7500
accuracy 0.74692
loss 2.35466

rawah:
f1_score 0.76853
auroc 0.92004
auprc 0.67794
recall 0.75785
precision 0.79110
confusion_matrix
     0      1      2      3      4
     1  78080  19098   1485   5992
     2  26808 109885   7465    905
     3     30    152   2743      0
     4    300     46      0   4210
accuracy 0.84135
loss 1.94743

saving test results to cover/log/89/seed_15//111000/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 16...(9/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_16//111319/20230409_personalizedfederated_cover_split_by_wilderness_model_16/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.975 valid 0.823 comanche:auroc: train 0.968 valid 0.936 comanche:auprc: train 0.836 valid 0.731 comanche:recall: train 0.975 valid 0.826 comanche:precision: train 0.975 valid 0.823 comanche:accuracy: train 0.972 valid 0.809 comanche:loss: train 0.094 valid 0.733 neota:f1_score: train 0.997 valid 0.902 neota:auroc: train 1.000 valid 0.955 neota:auprc: train 1.000 valid 0.874 neota:recall: train 0.997 valid 0.903 neota:precision: train 0.997 valid 0.902 neota:accuracy: train 0.998 valid 0.832 neota:loss: train 0.006 valid 0.902 poudre:f1_score: train 1.000 valid 0.908 poudre:auroc: train 0.997 valid 0.877 poudre:auprc: train 0.788 valid 0.716 poudre:recall: train 1.000 valid 0.909 poudre:precision: train 1.000 valid 0.909 poudre:accuracy: train 1.000 valid 0.759 poudre:loss: train 0.001 valid 0.584 rawah:f1_score: train 1.000 valid 0.841 rawah:auroc: train 0.997 valid 0.949 rawah:auprc: train 0.986 valid 0.890 rawah:recall: train 1.000 valid 0.842 rawah:precision: train 1.000 valid 0.840 rawah:accuracy: train 1.000 valid 0.863 rawah:loss: train 0.001 valid 1.325 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.69976
auroc 0.89811
auprc 0.57001
recall 0.68718
precision 0.74599
confusion_matrix
    0     1     2     3     4     5     6
    1 61248 16469   201  2131   558  6004
    2 22651 75519  4153 12935  7358  1537
    3    18   210 11589   291  1325     4
    4    27   177   106  4047    46     5
    5    24   140   380   109  6011     0
    6   368    20    16     8     0 11330
accuracy 0.82716
loss 1.35628

neota:
f1_score 0.79093
auroc 0.90772
auprc 0.87149
recall 0.79282
precision 0.79938
confusion_matrix
    0     1     2     3
    1 15620  1798   996
    2  2947  5626   346
    3     1     0  2051
accuracy 0.82619
loss 1.68774

poudre:
f1_score 0.80878
auroc 0.89253
auprc 0.70072
recall 0.81092
precision 0.84856
confusion_matrix
    0     1     2     3     4
    1   855  1465    36   650
    2    40 17224  1187  1706
    3     0     9   571     7
    4    26   643   337  7537
accuracy 0.74848
loss 3.30708

rawah:
f1_score 0.77700
auroc 0.91201
auprc 0.71887
recall 0.76659
precision 0.79893
confusion_matrix
     0      1      2      3      4
     1  78623  18920   1214   5898
     2  25100 111454   7632    877
     3     15    107   2803      0
     4    240     30      0   4286
accuracy 0.85465
loss 1.83340

saving test results to cover/log/89/seed_16//111319/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 17...(10/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_17//111638/20230409_personalizedfederated_cover_split_by_wilderness_model_17/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.979 valid 0.818 comanche:auroc: train 0.992 valid 0.945 comanche:auprc: train 0.948 valid 0.808 comanche:recall: train 0.979 valid 0.823 comanche:precision: train 0.979 valid 0.819 comanche:accuracy: train 0.978 valid 0.805 comanche:loss: train 0.068 valid 1.150 neota:f1_score: train 0.974 valid 0.837 neota:auroc: train 0.993 valid 0.936 neota:auprc: train 0.968 valid 0.800 neota:recall: train 0.974 valid 0.850 neota:precision: train 0.974 valid 0.841 neota:accuracy: train 0.958 valid 0.723 neota:loss: train 0.084 valid 0.813 poudre:f1_score: train 0.994 valid 0.897 poudre:auroc: train 0.999 valid 0.961 poudre:auprc: train 0.988 valid 0.794 poudre:recall: train 0.994 valid 0.898 poudre:precision: train 0.994 valid 0.898 poudre:accuracy: train 0.995 valid 0.710 poudre:loss: train 0.028 valid 0.549 rawah:f1_score: train 0.978 valid 0.803 rawah:auroc: train 0.987 valid 0.930 rawah:auprc: train 0.959 valid 0.845 rawah:recall: train 0.978 valid 0.806 rawah:precision: train 0.978 valid 0.805 rawah:accuracy: train 0.979 valid 0.834 rawah:loss: train 0.083 valid 1.088 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.71215
auroc 0.92908
auprc 0.67586
recall 0.69793
precision 0.76758
confusion_matrix
    0     1     2     3     4     5     6
    1 65630 12951   264  2580   538  4648
    2 22571 73445  4650 14621  7238  1628
    3     4   127 11775   491  1040     0
    4    26   104    82  4147    47     2
    5    10   112   428   147  5967     0
    6   272    28     0     5     1 11436
accuracy 0.83930
loss 2.18646

neota:
f1_score 0.74100
auroc 0.86515
auprc 0.82638
recall 0.75117
precision 0.75439
confusion_matrix
    0     1     2     3
    1 15726  1547  1141
    2  4367  4338   214
    3     9    34  2009
accuracy 0.77315
loss 1.02139

poudre:
f1_score 0.79580
auroc 0.89949
auprc 0.71078
recall 0.80473
precision 0.83625
confusion_matrix
    0     1     2     3     4
    1   628  1705    60   613
    2    53 17193  1024  1887
    3     0    13   566     8
    4    22   642   279  7600
accuracy 0.72893
loss 1.40199

rawah:
f1_score 0.77280
auroc 0.89921
auprc 0.63924
recall 0.75934
precision 0.80019
confusion_matrix
     0      1      2      3      4
     1  77551  17451   1570   8083
     2  25587 110662   7934    880
     3     17    125   2781      2
     4    225     23      0   4308
accuracy 0.85005
loss 1.58301

saving test results to cover/log/89/seed_17//111638/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 18...(11/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_18//111959/20230409_personalizedfederated_cover_split_by_wilderness_model_18/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.987 valid 0.831 comanche:auroc: train 0.986 valid 0.942 comanche:auprc: train 0.919 valid 0.779 comanche:recall: train 0.987 valid 0.834 comanche:precision: train 0.987 valid 0.830 comanche:accuracy: train 0.987 valid 0.817 comanche:loss: train 0.040 valid 1.002 neota:f1_score: train 1.000 valid 0.818 neota:auroc: train 0.995 valid 0.921 neota:auprc: train 0.934 valid 0.776 neota:recall: train 1.000 valid 0.832 neota:precision: train 1.000 valid 0.811 neota:accuracy: train 1.000 valid 0.689 neota:loss: train 0.004 valid 1.025 poudre:f1_score: train 0.997 valid 0.908 poudre:auroc: train 0.999 valid 0.924 poudre:auprc: train 0.997 valid 0.763 poudre:recall: train 0.997 valid 0.909 poudre:precision: train 0.997 valid 0.908 poudre:accuracy: train 0.998 valid 0.718 poudre:loss: train 0.008 valid 0.611 rawah:f1_score: train 0.984 valid 0.825 rawah:auroc: train 0.989 valid 0.940 rawah:auprc: train 0.969 valid 0.868 rawah:recall: train 0.984 valid 0.829 rawah:precision: train 0.984 valid 0.827 rawah:accuracy: train 0.984 valid 0.855 rawah:loss: train 0.048 valid 1.050 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.71901
auroc 0.91746
auprc 0.66022
recall 0.70943
precision 0.76258
confusion_matrix
    0     1     2     3     4     5     6
    1 65041 13962   173  2185   371  4879
    2 23767 76718  4925 10604  6844  1295
    3    12   128 11972   338   983     4
    4    43   172    85  4060    48     0
    5     7   118   480   131  5926     2
    6   195    18     0     5     0 11524
accuracy 0.84193
loss 1.69984

neota:
f1_score 0.75997
auroc 0.87346
auprc 0.79066
recall 0.76182
precision 0.77256
confusion_matrix
    0     1     2     3
    1 15137  1902  1375
    2  3370  5203   346
    3     2     4  2046
accuracy 0.80082
loss 1.53594

poudre:
f1_score 0.81299
auroc 0.90561
auprc 0.72870
recall 0.81454
precision 0.85311
confusion_matrix
    0     1     2     3     4
    1   887  1444    79   596
    2    39 17209  1155  1754
    3     0    10   568     9
    4    21   555   327  7640
accuracy 0.75269
loss 1.63428

rawah:
f1_score 0.78163
auroc 0.91598
auprc 0.71804
recall 0.77033
precision 0.80439
confusion_matrix
     0      1      2      3      4
     1  76121  20556   1431   6547
     2  21275 114877   8122    789
     3      4     77   2844      0
     4    236     35      0   4285
accuracy 0.85802
loss 1.16328

saving test results to cover/log/89/seed_18//111959/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 19...(12/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_19//112324/20230409_personalizedfederated_cover_split_by_wilderness_model_19/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.993 valid 0.838 comanche:auroc: train 0.995 valid 0.952 comanche:auprc: train 0.970 valid 0.828 comanche:recall: train 0.993 valid 0.842 comanche:precision: train 0.993 valid 0.837 comanche:accuracy: train 0.992 valid 0.825 comanche:loss: train 0.026 valid 1.121 neota:f1_score: train 0.952 valid 0.762 neota:auroc: train 0.982 valid 0.886 neota:auprc: train 0.906 valid 0.686 neota:recall: train 0.953 valid 0.752 neota:precision: train 0.952 valid 0.827 neota:accuracy: train 0.922 valid 0.751 neota:loss: train 0.176 valid 1.426 poudre:f1_score: train 0.994 valid 0.909 poudre:auroc: train 0.999 valid 0.974 poudre:auprc: train 0.998 valid 0.863 poudre:recall: train 0.994 valid 0.911 poudre:precision: train 0.994 valid 0.911 poudre:accuracy: train 0.995 valid 0.719 poudre:loss: train 0.017 valid 0.503 rawah:f1_score: train 0.999 valid 0.844 rawah:auroc: train 0.998 valid 0.949 rawah:auprc: train 0.994 valid 0.890 rawah:recall: train 0.999 valid 0.845 rawah:precision: train 0.999 valid 0.845 rawah:accuracy: train 0.999 valid 0.864 rawah:loss: train 0.005 valid 1.197 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.72562
auroc 0.93002
auprc 0.68293
recall 0.71290
precision 0.76854
confusion_matrix
    0     1     2     3     4     5     6
    1 64537 14657   242  2434   408  4333
    2 21497 78468  4151 12671  6426   940
    3    12   297 11688   399  1027    14
    4    29   155    73  4104    47     0
    5    27   144   467   137  5884     5
    6   259    53     3     6     4 11417
accuracy 0.83888
loss 2.00694

neota:
f1_score 0.76332
auroc 0.84206
auprc 0.77152
recall 0.76046
precision 0.77638
confusion_matrix
    0     1     2     3
    1 13885  3751   778
    2  2272  6439   208
    3    22     8  2022
accuracy 0.82046
loss 0.78831

poudre:
f1_score 0.80254
auroc 0.90787
auprc 0.77537
recall 0.80748
precision 0.84428
confusion_matrix
    0     1     2     3     4
    1   770  1546    62   628
    2    40 17167  1067  1883
    3     0     8   572     7
    4     9   652   315  7567
accuracy 0.74200
loss 1.42488

rawah:
f1_score 0.78340
auroc 0.92394
auprc 0.71884
recall 0.77300
precision 0.80482
confusion_matrix
     0      1      2      3      4
     1  76986  19820   1139   6710
     2  22143 114745   7099   1076
     3     15    117   2786      7
     4    211     46      0   4299
accuracy 0.85567
loss 1.73571

saving test results to cover/log/89/seed_19//112324/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 20...(13/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_20//112649/20230409_personalizedfederated_cover_split_by_wilderness_model_20/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.979 valid 0.817 comanche:auroc: train 0.978 valid 0.935 comanche:auprc: train 0.861 valid 0.738 comanche:recall: train 0.979 valid 0.821 comanche:precision: train 0.979 valid 0.818 comanche:accuracy: train 0.977 valid 0.802 comanche:loss: train 0.066 valid 0.978 neota:f1_score: train 0.982 valid 0.865 neota:auroc: train 0.996 valid 0.955 neota:auprc: train 0.992 valid 0.842 neota:recall: train 0.982 valid 0.876 neota:precision: train 0.982 valid 0.864 neota:accuracy: train 0.975 valid 0.745 neota:loss: train 0.054 valid 0.514 poudre:f1_score: train 0.997 valid 0.912 poudre:auroc: train 0.998 valid 0.920 poudre:auprc: train 0.995 valid 0.747 poudre:recall: train 0.997 valid 0.913 poudre:precision: train 0.997 valid 0.912 poudre:accuracy: train 0.997 valid 0.722 poudre:loss: train 0.012 valid 0.636 rawah:f1_score: train 1.000 valid 0.835 rawah:auroc: train 0.997 valid 0.947 rawah:auprc: train 0.989 valid 0.878 rawah:recall: train 1.000 valid 0.836 rawah:precision: train 1.000 valid 0.834 rawah:accuracy: train 1.000 valid 0.857 rawah:loss: train 0.001 valid 1.409 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.71269
auroc 0.90913
auprc 0.65984
recall 0.70241
precision 0.76699
confusion_matrix
    0     1     2     3     4     5     6
    1 68168 11625   187  2092   487  4052
    2 26934 72407  3811 11644  8461   896
    3    25   232 11354   359  1462     5
    4    35   151    74  4093    54     1
    5    14    95   354    91  6090    20
    6   321    14     3     5     6 11393
accuracy 0.83799
loss 1.53087

neota:
f1_score 0.78471
auroc 0.90273
auprc 0.85701
recall 0.79034
precision 0.79270
confusion_matrix
    0     1     2     3
    1 15985  1564   865
    2  3500  5195   224
    3     5     3  2044
accuracy 0.81555
loss 0.98938

poudre:
f1_score 0.81400
auroc 0.87456
auprc 0.73640
recall 0.81962
precision 0.84872
confusion_matrix
    0     1     2     3     4
    1   766  1572   124   544
    2    84 17548  1008  1517
    3     0     9   568    10
    4    24   637   296  7586
accuracy 0.74525
loss 1.92748

rawah:
f1_score 0.78006
auroc 0.91342
auprc 0.72612
recall 0.77031
precision 0.80134
confusion_matrix
     0      1      2      3      4
     1  79684  17974   1163   5834
     2  25863 111379   7071    750
     3     12    119   2794      0
     4    262     27      0   4267
accuracy 0.85524
loss 3.40239

saving test results to cover/log/89/seed_20//112649/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 21...(14/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_21//113016/20230409_personalizedfederated_cover_split_by_wilderness_model_21/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.989 valid 0.837 comanche:auroc: train 0.989 valid 0.949 comanche:auprc: train 0.939 valid 0.820 comanche:recall: train 0.989 valid 0.840 comanche:precision: train 0.989 valid 0.835 comanche:accuracy: train 0.988 valid 0.823 comanche:loss: train 0.042 valid 1.157 neota:f1_score: train 0.997 valid 0.896 neota:auroc: train 0.996 valid 0.936 neota:auprc: train 0.958 valid 0.857 neota:recall: train 0.997 valid 0.903 neota:precision: train 0.997 valid 0.898 neota:accuracy: train 0.993 valid 0.801 neota:loss: train 0.005 valid 0.842 poudre:f1_score: train 0.998 valid 0.914 poudre:auroc: train 1.000 valid 0.955 poudre:auprc: train 0.999 valid 0.826 poudre:recall: train 0.998 valid 0.915 poudre:precision: train 0.998 valid 0.913 poudre:accuracy: train 0.998 valid 0.724 poudre:loss: train 0.006 valid 0.545 rawah:f1_score: train 0.988 valid 0.824 rawah:auroc: train 0.978 valid 0.923 rawah:auprc: train 0.928 valid 0.817 rawah:recall: train 0.988 valid 0.825 rawah:precision: train 0.988 valid 0.824 rawah:accuracy: train 0.989 valid 0.849 rawah:loss: train 0.040 valid 1.005 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.71796
auroc 0.92543
auprc 0.66405
recall 0.70802
precision 0.76155
confusion_matrix
    0     1     2     3     4     5     6
    1 64436 14025   367  2020   402  5361
    2 23516 77065  4535 11045  6257  1735
    3    18   182 11929   305   992    11
    4    46   163    95  4039    63     2
    5    11   117   491   150  5890     5
    6   178    25     2     4     0 11533
accuracy 0.83914
loss 2.25559

neota:
f1_score 0.78504
auroc 0.90195
auprc 0.86563
recall 0.78799
precision 0.79854
confusion_matrix
    0     1     2     3
    1 15777  1423  1214
    2  3264  5329   326
    3     0     3  2049
accuracy 0.81761
loss 1.53285

poudre:
f1_score 0.81032
auroc 0.88225
auprc 0.73241
recall 0.81361
precision 0.85041
confusion_matrix
    0     1     2     3     4
    1   795  1607    87   517
    2    45 17316  1203  1593
    3     0     9   567    11
    4    22   613   312  7596
accuracy 0.74465
loss 1.60473

rawah:
f1_score 0.77406
auroc 0.90566
auprc 0.68950
recall 0.76335
precision 0.79696
confusion_matrix
     0      1      2      3      4
     1  77046  18545    858   8206
     2  25658 112160   6263    982
     3     16    115   2794      0
     4    178     42      2   4334
accuracy 0.85396
loss 1.31131

saving test results to cover/log/89/seed_21//113016/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 22...(15/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_22//113336/20230409_personalizedfederated_cover_split_by_wilderness_model_22/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.991 valid 0.824 comanche:auroc: train 0.987 valid 0.936 comanche:auprc: train 0.878 valid 0.713 comanche:recall: train 0.991 valid 0.828 comanche:precision: train 0.991 valid 0.824 comanche:accuracy: train 0.990 valid 0.811 comanche:loss: train 0.031 valid 1.149 neota:f1_score: train 0.997 valid 0.839 neota:auroc: train 0.995 valid 0.921 neota:auprc: train 0.946 valid 0.768 neota:recall: train 0.997 valid 0.850 neota:precision: train 0.997 valid 0.834 neota:accuracy: train 0.993 valid 0.721 neota:loss: train 0.006 valid 0.941 poudre:f1_score: train 0.991 valid 0.912 poudre:auroc: train 0.998 valid 0.874 poudre:auprc: train 0.994 valid 0.763 poudre:recall: train 0.991 valid 0.913 poudre:precision: train 0.991 valid 0.912 poudre:accuracy: train 0.992 valid 0.722 poudre:loss: train 0.024 valid 0.487 rawah:f1_score: train 1.000 valid 0.833 rawah:auroc: train 0.998 valid 0.943 rawah:auprc: train 0.989 valid 0.871 rawah:recall: train 1.000 valid 0.834 rawah:precision: train 1.000 valid 0.832 rawah:accuracy: train 1.000 valid 0.857 rawah:loss: train 0.001 valid 1.510 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.71344
auroc 0.92292
auprc 0.63190
recall 0.70138
precision 0.76172
confusion_matrix
    0     1     2     3     4     5     6
    1 64140 13896   251  2149   602  5573
    2 22995 75925  4478 12230  7161  1364
    3    16   145 11677   331  1244    24
    4    59   125    68  4102    54     0
    5     3   135   434   158  5931     3
    6   256     5     1     4     0 11476
accuracy 0.83651
loss 1.99191

neota:
f1_score 0.77599
auroc 0.90251
auprc 0.83434
recall 0.77853
precision 0.78606
confusion_matrix
    0     1     2     3
    1 15478  1809  1127
    2  3218  5353   348
    3     6     0  2046
accuracy 0.81260
loss 1.38399

poudre:
f1_score 0.80430
auroc 0.88308
auprc 0.73169
recall 0.81160
precision 0.84948
confusion_matrix
    0     1     2     3     4
    1   672  1567    53   714
    2    17 17323  1116  1701
    3     0     7   572     8
    4    14   603   284  7642
accuracy 0.73798
loss 1.48879

rawah:
f1_score 0.76984
auroc 0.91414
auprc 0.67351
recall 0.75808
precision 0.79483
confusion_matrix
     0      1      2      3      4
     1  78732  18214   1415   6294
     2  26695 109215   7927   1226
     3     16    119   2790      0
     4    272     41      2   4241
accuracy 0.84747
loss 94.91207

saving test results to cover/log/89/seed_22//113336/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 23...(16/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_23//113657/20230409_personalizedfederated_cover_split_by_wilderness_model_23/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.966 valid 0.830 comanche:auroc: train 0.978 valid 0.946 comanche:auprc: train 0.880 valid 0.780 comanche:recall: train 0.966 valid 0.834 comanche:precision: train 0.966 valid 0.829 comanche:accuracy: train 0.963 valid 0.815 comanche:loss: train 0.104 valid 0.623 neota:f1_score: train 1.000 valid 0.890 neota:auroc: train 1.000 valid 0.925 neota:auprc: train 1.000 valid 0.838 neota:recall: train 1.000 valid 0.894 neota:precision: train 1.000 valid 0.888 neota:accuracy: train 1.000 valid 0.808 neota:loss: train 0.002 valid 1.094 poudre:f1_score: train 0.999 valid 0.905 poudre:auroc: train 0.999 valid 0.916 poudre:auprc: train 0.996 valid 0.769 poudre:recall: train 0.999 valid 0.907 poudre:precision: train 0.999 valid 0.905 poudre:accuracy: train 0.999 valid 0.715 poudre:loss: train 0.005 valid 0.552 rawah:f1_score: train 0.981 valid 0.829 rawah:auroc: train 0.995 valid 0.948 rawah:auprc: train 0.981 valid 0.887 rawah:recall: train 0.981 valid 0.830 rawah:precision: train 0.980 valid 0.830 rawah:accuracy: train 0.983 valid 0.848 rawah:loss: train 0.056 valid 0.980 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.73255
auroc 0.91520
auprc 0.64415
recall 0.72077
precision 0.76761
confusion_matrix
    0     1     2     3     4     5     6
    1 61563 17489    94  2316   361  4788
    2 18529 83554  3860 10779  6511   920
    3    20   219 11630   403  1165     0
    4    22   236    84  4005    61     0
    5    20   137   445   146  5916     0
    6   294    59     8     5     2 11374
accuracy 0.83572
loss 1.17614

neota:
f1_score 0.77679
auroc 0.89638
auprc 0.84741
recall 0.78033
precision 0.78759
confusion_matrix
    0     1     2     3
    1 15645  1672  1097
    2  3326  5237   356
    3     4     0  2048
accuracy 0.81162
loss 1.64784

poudre:
f1_score 0.79739
auroc 0.86764
auprc 0.67352
recall 0.80346
precision 0.84352
confusion_matrix
    0     1     2     3     4
    1   624  1718    99   565
    2    50 17135  1244  1728
    3     0     9   571     7
    4     6   573   348  7616
accuracy 0.73047
loss 2.15840

rawah:
f1_score 0.77318
auroc 0.91819
auprc 0.70448
recall 0.76187
precision 0.79631
confusion_matrix
     0      1      2      3      4
     1  76926  18999   1540   7190
     2  25101 112041   7090    831
     3     14    156   2753      2
     4    281     35      8   4232
accuracy 0.84437
loss 1.22089

saving test results to cover/log/89/seed_23//113657/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 24...(17/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_24//114020/20230409_personalizedfederated_cover_split_by_wilderness_model_24/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.978 valid 0.824 comanche:auroc: train 0.987 valid 0.940 comanche:auprc: train 0.922 valid 0.784 comanche:recall: train 0.978 valid 0.828 comanche:precision: train 0.978 valid 0.823 comanche:accuracy: train 0.977 valid 0.808 comanche:loss: train 0.072 valid 0.989 neota:f1_score: train 0.968 valid 0.876 neota:auroc: train 0.989 valid 0.938 neota:auprc: train 0.944 valid 0.817 neota:recall: train 0.968 valid 0.876 neota:precision: train 0.968 valid 0.879 neota:accuracy: train 0.942 valid 0.826 neota:loss: train 0.202 valid 0.677 poudre:f1_score: train 0.998 valid 0.913 poudre:auroc: train 0.997 valid 0.895 poudre:auprc: train 0.990 valid 0.780 poudre:recall: train 0.998 valid 0.914 poudre:precision: train 0.998 valid 0.913 poudre:accuracy: train 0.999 valid 0.763 poudre:loss: train 0.004 valid 0.542 rawah:f1_score: train 1.000 valid 0.846 rawah:auroc: train 0.996 valid 0.943 rawah:auprc: train 0.990 valid 0.874 rawah:recall: train 1.000 valid 0.848 rawah:precision: train 1.000 valid 0.847 rawah:accuracy: train 1.000 valid 0.869 rawah:loss: train 0.001 valid 1.300 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.71134
auroc 0.92271
auprc 0.66213
recall 0.69800
precision 0.75983
confusion_matrix
    0     1     2     3     4     5     6
    1 63219 14918   175  2574   454  5271
    2 21865 75852  4292 13149  7451  1544
    3     6   170 11809   345  1104     3
    4    30   145    71  4095    65     2
    5    12   146   461    99  5946     0
    6   213    27     0     2     4 11496
accuracy 0.83667
loss 1.77534

neota:
f1_score 0.76951
auroc 0.87700
auprc 0.83086
recall 0.77036
precision 0.77626
confusion_matrix
    0     1     2     3
    1 14960  2412  1042
    2  3016  5639   264
    3    10     4  2038
accuracy 0.81262
loss 1.37928

poudre:
f1_score 0.81779
auroc 0.86647
auprc 0.63344
recall 0.82083
precision 0.85378
confusion_matrix
    0     1     2     3     4
    1   868  1549    74   515
    2    71 17398  1111  1577
    3     0    13   563    11
    4    17   551   297  7678
accuracy 0.75244
loss 2.96673

rawah:
f1_score 0.77289
auroc 0.90334
auprc 0.70491
recall 0.76134
precision 0.79646
confusion_matrix
     0      1      2      3      4
     1  76950  19180   1332   7193
     2  24783 111864   7434    982
     3     13    107   2805      0
     4    315     43      1   4197
accuracy 0.84665
loss 1.85625

saving test results to cover/log/89/seed_24//114020/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 25...(18/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_25//114356/20230409_personalizedfederated_cover_split_by_wilderness_model_25/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.995 valid 0.833 comanche:auroc: train 0.988 valid 0.947 comanche:auprc: train 0.924 valid 0.795 comanche:recall: train 0.995 valid 0.835 comanche:precision: train 0.995 valid 0.832 comanche:accuracy: train 0.994 valid 0.819 comanche:loss: train 0.020 valid 1.212 neota:f1_score: train 0.949 valid 0.869 neota:auroc: train 0.974 valid 0.912 neota:auprc: train 0.910 valid 0.774 neota:recall: train 0.950 valid 0.867 neota:precision: train 0.950 valid 0.878 neota:accuracy: train 0.919 valid 0.818 neota:loss: train 0.168 valid 0.827 poudre:f1_score: train 1.000 valid 0.913 poudre:auroc: train 1.000 valid 0.928 poudre:auprc: train 0.999 valid 0.771 poudre:recall: train 1.000 valid 0.914 poudre:precision: train 1.000 valid 0.915 poudre:accuracy: train 1.000 valid 0.723 poudre:loss: train 0.001 valid 0.633 rawah:f1_score: train 1.000 valid 0.830 rawah:auroc: train 0.999 valid 0.950 rawah:auprc: train 0.998 valid 0.895 rawah:recall: train 1.000 valid 0.832 rawah:precision: train 1.000 valid 0.830 rawah:accuracy: train 1.000 valid 0.855 rawah:loss: train 0.001 valid 1.335 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.72213
auroc 0.91921
auprc 0.63953
recall 0.70940
precision 0.76072
confusion_matrix
    0     1     2     3     4     5     6
    1 60650 17287   282  2686   600  5106
    2 19329 81587  3955 11681  6553  1048
    3    24   298 11714   249  1143     9
    4    46   191    84  4035    43     9
    5    11   196   448   138  5871     0
    6   302    58     0     6     0 11376
accuracy 0.83240
loss 2.14216

neota:
f1_score 0.76021
auroc 0.85239
auprc 0.76132
recall 0.75971
precision 0.77339
confusion_matrix
    0     1     2     3
    1 14807  2143  1464
    2  3082  5476   361
    3     4     7  2041
accuracy 0.80424
loss 0.88459

poudre:
f1_score 0.79968
auroc 0.89140
auprc 0.73004
recall 0.80513
precision 0.84349
confusion_matrix
    0     1     2     3     4
    1   697  1544    47   718
    2    44 17146  1228  1739
    3     0     5   576     6
    4    13   663   286  7581
accuracy 0.73779
loss 2.13729

rawah:
f1_score 0.77903
auroc 0.91914
auprc 0.72511
recall 0.76750
precision 0.80307
confusion_matrix
     0      1      2      3      4
     1  79004  17721   1315   6615
     2  25097 111345   7672    949
     3     13    113   2799      0
     4    274     29      2   4251
accuracy 0.85311
loss 1.95080

saving test results to cover/log/89/seed_25//114356/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 26...(19/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_26//114722/20230409_personalizedfederated_cover_split_by_wilderness_model_26/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.990 valid 0.839 comanche:auroc: train 0.985 valid 0.944 comanche:auprc: train 0.901 valid 0.775 comanche:recall: train 0.990 valid 0.842 comanche:precision: train 0.990 valid 0.839 comanche:accuracy: train 0.989 valid 0.826 comanche:loss: train 0.031 valid 0.943 neota:f1_score: train 0.997 valid 0.869 neota:auroc: train 1.000 valid 0.923 neota:auprc: train 1.000 valid 0.827 neota:recall: train 0.997 valid 0.876 neota:precision: train 0.997 valid 0.866 neota:accuracy: train 0.998 valid 0.761 neota:loss: train 0.003 valid 1.400 poudre:f1_score: train 0.994 valid 0.909 poudre:auroc: train 0.999 valid 0.909 poudre:auprc: train 0.998 valid 0.761 poudre:recall: train 0.994 valid 0.911 poudre:precision: train 0.994 valid 0.912 poudre:accuracy: train 0.996 valid 0.720 poudre:loss: train 0.024 valid 0.523 rawah:f1_score: train 0.999 valid 0.835 rawah:auroc: train 1.000 valid 0.953 rawah:auprc: train 0.998 valid 0.899 rawah:recall: train 0.999 valid 0.837 rawah:precision: train 0.999 valid 0.838 rawah:accuracy: train 0.999 valid 0.857 rawah:loss: train 0.001 valid 1.322 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.72285
auroc 0.92348
auprc 0.64093
recall 0.71046
precision 0.76335
confusion_matrix
    0     1     2     3     4     5     6
    1 62594 16292   274  2320   600  4531
    2 19918 79666  4622 12274  6480  1193
    3    29   244 12023   291   839    11
    4    36   179    98  4047    47     1
    5     4   134   633   133  5758     2
    6   305    20     3     7     0 11407
accuracy 0.83546
loss 2.01565

neota:
f1_score 0.76811
auroc 0.88847
auprc 0.85501
recall 0.77349
precision 0.77907
confusion_matrix
    0     1     2     3
    1 15690  1615  1109
    2  3654  4998   267
    3     8     3  2041
accuracy 0.80236
loss 2.14006

poudre:
f1_score 0.81257
auroc 0.89824
auprc 0.74267
recall 0.81878
precision 0.84976
confusion_matrix
    0     1     2     3     4
    1   788  1439    65   714
    2    50 17411   954  1742
    3     0    12   562    13
    4    23   586   254  7680
accuracy 0.74558
loss 1.56295

rawah:
f1_score 0.77614
auroc 0.91964
auprc 0.69188
recall 0.76535
precision 0.79847
confusion_matrix
     0      1      2      3      4
     1  77206  19355   1195   6899
     2  24263 112602   7145   1053
     3     16     92   2817      0
     4    287     48      0   4221
accuracy 0.85087
loss 2.01179

saving test results to cover/log/89/seed_26//114722/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 27...(20/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_27//115049/20230409_personalizedfederated_cover_split_by_wilderness_model_27/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.979 valid 0.849 comanche:auroc: train 0.978 valid 0.950 comanche:auprc: train 0.870 valid 0.805 comanche:recall: train 0.979 valid 0.852 comanche:precision: train 0.979 valid 0.847 comanche:accuracy: train 0.977 valid 0.835 comanche:loss: train 0.064 valid 0.664 neota:f1_score: train 0.989 valid 0.881 neota:auroc: train 0.999 valid 0.946 neota:auprc: train 0.995 valid 0.852 neota:recall: train 0.989 valid 0.885 neota:precision: train 0.989 valid 0.879 neota:accuracy: train 0.982 valid 0.800 neota:loss: train 0.029 valid 0.687 poudre:f1_score: train 0.996 valid 0.899 poudre:auroc: train 0.999 valid 0.939 poudre:auprc: train 0.991 valid 0.777 poudre:recall: train 0.996 valid 0.900 poudre:precision: train 0.996 valid 0.898 poudre:accuracy: train 0.997 valid 0.750 poudre:loss: train 0.015 valid 0.411 rawah:f1_score: train 0.993 valid 0.843 rawah:auroc: train 0.992 valid 0.940 rawah:auprc: train 0.973 valid 0.853 rawah:recall: train 0.993 valid 0.845 rawah:precision: train 0.993 valid 0.844 rawah:accuracy: train 0.994 valid 0.862 rawah:loss: train 0.034 valid 1.010 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.72665
auroc 0.90828
auprc 0.62460
recall 0.71459
precision 0.77293
confusion_matrix
    0     1     2     3     4     5     6
    1 66030 13188   307  2429   402  4255
    2 22863 77010  4127 12641  6728   784
    3     4   221 11946   250  1012     4
    4    30   124   100  4122    32     0
    5     7   100   426   137  5994     0
    6   299    25     0     5     0 11413
accuracy 0.84638
loss 1.31134

neota:
f1_score 0.77871
auroc 0.89181
auprc 0.85754
recall 0.78428
precision 0.78711
confusion_matrix
    0     1     2     3
    1 15892  1663   859
    2  3475  5108   336
    3     6     0  2046
accuracy 0.81094
loss 1.01719

poudre:
f1_score 0.81984
auroc 0.89730
auprc 0.74189
recall 0.81779
precision 0.85810
confusion_matrix
    0     1     2     3     4
    1  1037  1423    94   452
    2    32 17280  1242  1603
    3     0     9   570     8
    4    17   652   352  7522
accuracy 0.76344
loss 1.38610

rawah:
f1_score 0.78574
auroc 0.90543
auprc 0.68209
recall 0.77477
precision 0.80862
confusion_matrix
     0      1      2      3      4
     1  79585  17195   1451   6424
     2  24376 112649   7069    969
     3     11    128   2786      0
     4    269     38      0   4249
accuracy 0.85552
loss 1.32230

saving test results to cover/log/89/seed_27//115049/test_results.txt
Done personalizedfederated testing.
Loading personalizedfederated trainer for seed 28...(21/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to True; loading PartiallyPersonalizedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_28//115415/20230409_personalizedfederated_cover_split_by_wilderness_model_28/
Starting personalizedfederated training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.996 valid 0.839 comanche:auroc: train 0.994 valid 0.950 comanche:auprc: train 0.967 valid 0.816 comanche:recall: train 0.996 valid 0.841 comanche:precision: train 0.996 valid 0.838 comanche:accuracy: train 0.996 valid 0.826 comanche:loss: train 0.012 valid 1.284 neota:f1_score: train 0.997 valid 0.871 neota:auroc: train 1.000 valid 0.931 neota:auprc: train 1.000 valid 0.825 neota:recall: train 0.997 valid 0.876 neota:precision: train 0.997 valid 0.868 neota:accuracy: train 0.994 valid 0.776 neota:loss: train 0.004 valid 0.938 poudre:f1_score: train 0.999 valid 0.912 poudre:auroc: train 0.999 valid 0.940 poudre:auprc: train 0.998 valid 0.808 poudre:recall: train 0.999 valid 0.913 poudre:precision: train 0.999 valid 0.913 poudre:accuracy: train 1.000 valid 0.762 poudre:loss: train 0.001 valid 0.617 rawah:f1_score: train 1.000 valid 0.821 rawah:auroc: train 0.999 valid 0.953 rawah:auprc: train 0.999 valid 0.897 rawah:recall: train 1.000 valid 0.823 rawah:precision: train 1.000 valid 0.821 rawah:accuracy: train 1.000 valid 0.844 rawah:loss: train 0.003 valid 1.269 

Done personalizedfederated training. Starting testing....
Test results:

comanche:
f1_score 0.72728
auroc 0.92592
auprc 0.65406
recall 0.71515
precision 0.76847
confusion_matrix
    0     1     2     3     4     5     6
    1 63864 15010   291  2258   616  4572
    2 21113 79495  3912 11955  6748   930
    3    55   273 11819   271  1004    15
    4    23   185   102  4048    47     3
    5    14   136   450   121  5942     1
    6   225    31     0     2     0 11484
accuracy 0.84088
loss 2.37850

neota:
f1_score 0.76817
auroc 0.88811
auprc 0.82708
recall 0.77414
precision 0.77538
confusion_matrix
    0     1     2     3
    1 15720  1819   875
    2  3635  4989   295
    3     8     5  2039
accuracy 0.80224
loss 1.40839

poudre:
f1_score 0.79944
auroc 0.88605
auprc 0.70847
recall 0.80411
precision 0.84727
confusion_matrix
    0     1     2     3     4
    1   676  1444    76   810
    2    30 17004  1264  1859
    3     0     7   569    11
    4    24   497   304  7718
accuracy 0.73531
loss 2.36358

rawah:
f1_score 0.77420
auroc 0.91875
auprc 0.72795
recall 0.76324
precision 0.79715
confusion_matrix
     0      1      2      3      4
     1  78377  18471   1476   6331
     2  25883 110916   7460    804
     3      8    145   2772      0
     4    268     48      1   4239
accuracy 0.84791
loss 1.82429

saving test results to cover/log/89/seed_28//115415/test_results.txt
Done personalizedfederated testing.
WARNING. With option --federate it is impossible to not --personalize without also --force-consistent-target-space.Otherwise output layer params will be mismatched and cannot be federated.
Overriding with --force-consistent-target-space set as True.
Running for seeds [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]...
Loading federatedsame_yspace trainer for seed 8...(1/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_8//115742/20230409_federatedsame_yspace_cover_split_by_wilderness_model_8/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.990 valid 0.818 comanche:auroc: train 0.810 valid 0.767 comanche:auprc: train 0.468 valid 0.406 comanche:recall: train 0.990 valid 0.823 comanche:precision: train 0.990 valid 0.817 comanche:accuracy: train 0.989 valid 0.803 comanche:loss: train 0.037 valid 1.052 neota:f1_score: train 1.000 valid 0.841 neota:auroc: train 0.639 valid 0.593 neota:auprc: train 0.281 valid 0.240 neota:recall: train 1.000 valid 0.805 neota:precision: train 1.000 valid 0.887 neota:accuracy: train 1.000 valid 0.714 neota:loss: train 0.002 valid 1.277 poudre:f1_score: train 0.997 valid 0.909 poudre:auroc: train 0.676 valid 0.638 poudre:auprc: train 0.423 valid 0.306 poudre:recall: train 0.997 valid 0.906 poudre:precision: train 0.997 valid 0.914 poudre:accuracy: train 0.997 valid 0.752 poudre:loss: train 0.016 valid 0.523 rawah:f1_score: train 0.997 valid 0.825 rawah:auroc: train 0.728 valid 0.690 rawah:auprc: train 0.446 valid 0.377 rawah:recall: train 0.997 valid 0.826 rawah:precision: train 0.997 valid 0.825 rawah:accuracy: train 0.998 valid 0.849 rawah:loss: train 0.009 valid 1.006 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.67594
auroc 0.78041
auprc 0.39098
recall 0.66181
precision 0.73824
confusion_matrix
    0     1     2     3     4     5     6     7
    1 61366 14003  1142   506  2099  1380  6115
    2 24635 69034  6687  1720 11990  8425  1662
    3    11   101 11851   116   291  1064     3
    4     0     0     0     0     0     0     0
    5    37   144   104     7  4052    59     5
    6    39   122   540    69   131  5754     9
    7   282    12     8     5    10     5 11420
accuracy 0.81697
loss 2.01621

neota:
f1_score 0.71032
auroc 0.60351
auprc 0.27859
recall 0.65666
precision 0.78242
confusion_matrix
    0     1     2     3     4     5     6     7
    1 12108  2692   688   514  1110   724   578
    2  2175  5185   377   384   374   325    99
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7    19     3     0    20     0     7  2003
accuracy 0.73834
loss 2.18799

poudre:
f1_score 0.78898
auroc 0.59046
auprc 0.25592
recall 0.76914
precision 0.82842
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   160  1025   864    44    88   615   210
    3   407   702 15794   883   243  1984   144
    4     0     2     6   565     1    13     0
    5     0     0     0     0     0     0     0
    6   142   208   413   229    20  7454    77
    7     0     0     0     0     0     0     0
accuracy 0.73990
loss 1.43326

rawah:
f1_score 0.75629
auroc 0.68628
auprc 0.29538
recall 0.73861
precision 0.78936
confusion_matrix
     0      1      2      3      4      5      6      7
     1  74938  18530    408    220   1269    279   9011
     2  26258 107928    337    167   8434    903   1036
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     28    112      1      2   2778      4      0
     6      0      0      0      0      0      0      0
     7    164     50     12      3      0      2   4325
accuracy 0.83977
loss 1.54742

saving test results to cover/log/89/seed_8//115742/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 9...(2/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_9//120117/20230409_federatedsame_yspace_cover_split_by_wilderness_model_9/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.993 valid 0.841 comanche:auroc: train 0.884 valid 0.834 comanche:auprc: train 0.671 valid 0.565 comanche:recall: train 0.993 valid 0.844 comanche:precision: train 0.993 valid 0.841 comanche:accuracy: train 0.992 valid 0.826 comanche:loss: train 0.026 valid 0.927 neota:f1_score: train 0.995 valid 0.843 neota:auroc: train 0.644 valid 0.585 neota:auprc: train 0.312 valid 0.250 neota:recall: train 0.995 valid 0.823 neota:precision: train 0.995 valid 0.872 neota:accuracy: train 0.995 valid 0.728 neota:loss: train 0.011 valid 1.092 poudre:f1_score: train 0.982 valid 0.886 poudre:auroc: train 0.744 valid 0.669 poudre:auprc: train 0.468 valid 0.327 poudre:recall: train 0.982 valid 0.886 poudre:precision: train 0.982 valid 0.889 poudre:accuracy: train 0.985 valid 0.739 poudre:loss: train 0.065 valid 0.553 rawah:f1_score: train 0.996 valid 0.833 rawah:auroc: train 0.764 valid 0.730 rawah:auprc: train 0.521 valid 0.461 rawah:recall: train 0.996 valid 0.835 rawah:precision: train 0.997 valid 0.833 rawah:accuracy: train 0.996 valid 0.858 rawah:loss: train 0.011 valid 1.102 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.68625
auroc 0.83481
auprc 0.49604
recall 0.67025
precision 0.74708
confusion_matrix
    0     1     2     3     4     5     6     7
    1 61060 14009  1311   405  2509  1954  5363
    2 22072 71550  7222   970 12760  8084  1495
    3    17   149 11877    65   287  1030    12
    4     0     0     0     0     0     0     0
    5    35   193   100     0  4049    28     3
    6    34   165   621    66   149  5628     1
    7   258    33    12     3     3    35 11398
accuracy 0.81650
loss 1.94587

neota:
f1_score 0.70614
auroc 0.61572
auprc 0.31234
recall 0.65751
precision 0.77092
confusion_matrix
    0     1     2     3     4     5     6     7
    1 12249  2874   751   368   898   709   565
    2  2296  5086   380   307   358   327   165
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7    39    16     1     3     3     4  1986
accuracy 0.73443
loss 2.22534

poudre:
f1_score 0.78753
auroc 0.67094
auprc 0.29094
recall 0.77621
precision 0.82349
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   104   934  1136   117     5   675    35
    3   252   541 15890   981   283  2130    80
    4     0     0    14   562     0    11     0
    5     0     0     0     0     0     0     0
    6   103    99   384   215    22  7680    40
    7     0     0     0     0     0     0     0
accuracy 0.73885
loss 1.40765

rawah:
f1_score 0.75507
auroc 0.71936
auprc 0.35305
recall 0.73643
precision 0.78873
confusion_matrix
     0      1      2      3      4      5      6      7
     1  75517  19112    343    215   1703    372   7393
     2  25717 106916    616    236   9734    878    966
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     22     91      8      0   2800      4      0
     6      0      0      0      0      0      0      0
     7    303     29     24     11      0     13   4176
accuracy 0.83312
loss 1.62867

saving test results to cover/log/89/seed_9//120117/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 10...(3/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_10//120442/20230409_federatedsame_yspace_cover_split_by_wilderness_model_10/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.963 valid 0.828 comanche:auroc: train 0.879 valid 0.856 comanche:auprc: train 0.635 valid 0.574 comanche:recall: train 0.963 valid 0.831 comanche:precision: train 0.963 valid 0.827 comanche:accuracy: train 0.960 valid 0.814 comanche:loss: train 0.116 valid 0.786 neota:f1_score: train 0.979 valid 0.884 neota:auroc: train 0.672 valid 0.637 neota:auprc: train 0.356 valid 0.296 neota:recall: train 0.974 valid 0.885 neota:precision: train 0.984 valid 0.887 neota:accuracy: train 0.969 valid 0.800 neota:loss: train 0.107 valid 0.681 poudre:f1_score: train 0.969 valid 0.890 poudre:auroc: train 0.747 valid 0.678 poudre:auprc: train 0.484 valid 0.346 poudre:recall: train 0.969 valid 0.892 poudre:precision: train 0.970 valid 0.895 poudre:accuracy: train 0.939 valid 0.705 poudre:loss: train 0.110 valid 0.533 rawah:f1_score: train 0.974 valid 0.820 rawah:auroc: train 0.765 valid 0.739 rawah:auprc: train 0.528 valid 0.480 rawah:recall: train 0.972 valid 0.822 rawah:precision: train 0.975 valid 0.821 rawah:accuracy: train 0.975 valid 0.845 rawah:loss: train 0.080 valid 0.913 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.68818
auroc 0.81924
auprc 0.44823
recall 0.67813
precision 0.73631
confusion_matrix
    0     1     2     3     4     5     6     7
    1 62720 14395   563   129  2207   600  5997
    2 26607 72310  6080   845 10752  5948  1611
    3    92   280 12066    78   242   674     5
    4     0     0     0     0     0     0     0
    5    74   317   120     8  3848    37     4
    6    46   328   867    43   160  5219     1
    7   339    28     4     0     6    19 11346
accuracy 0.80449
loss 1.19479

neota:
f1_score 0.73658
auroc 0.62836
auprc 0.31101
recall 0.70679
precision 0.77715
confusion_matrix
    0     1     2     3     4     5     6     7
    1 13090  3025   436   238   626   299   700
    2  2358  5665   358   118   229    29   162
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7    10    12     0     2     0    14  2014
accuracy 0.77584
loss 1.21598

poudre:
f1_score 0.74391
auroc 0.65816
auprc 0.26282
recall 0.69080
precision 0.82097
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   235  1262   673    29    11   284   512
    3   931  1518 14264   818   313  1773   540
    4     7    11    11   541     3    14     0
    5     0     0     0     0     0     0     0
    6   833   455   400   239    58  6241   317
    7     0     0     0     0     0     0     0
accuracy 0.69491
loss 1.29631

rawah:
f1_score 0.73642
auroc 0.70892
auprc 0.33039
recall 0.71712
precision 0.77648
confusion_matrix
     0      1      2      3      4      5      6      7
     1  75895  17137     96     69   1367    188   9903
     2  31429 101418    628     94   9439   1008   1047
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     24    108      0      0   2791      2      0
     6      0      0      0      0      0      0      0
     7    178     32      6      0      0      1   4339
accuracy 0.83272
loss 1.07813

saving test results to cover/log/89/seed_10//120442/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 11...(4/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_11//120812/20230409_federatedsame_yspace_cover_split_by_wilderness_model_11/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.980 valid 0.824 comanche:auroc: train 0.883 valid 0.831 comanche:auprc: train 0.662 valid 0.550 comanche:recall: train 0.980 valid 0.826 comanche:precision: train 0.980 valid 0.826 comanche:accuracy: train 0.979 valid 0.806 comanche:loss: train 0.068 valid 0.889 neota:f1_score: train 0.995 valid 0.851 neota:auroc: train 0.669 valid 0.629 neota:auprc: train 0.326 valid 0.285 neota:recall: train 0.995 valid 0.850 neota:precision: train 0.995 valid 0.856 neota:accuracy: train 0.991 valid 0.752 neota:loss: train 0.015 valid 0.808 poudre:f1_score: train 0.990 valid 0.895 poudre:auroc: train 0.740 valid 0.716 poudre:auprc: train 0.467 valid 0.338 poudre:recall: train 0.990 valid 0.893 poudre:precision: train 0.991 valid 0.898 poudre:accuracy: train 0.993 valid 0.743 poudre:loss: train 0.034 valid 0.523 rawah:f1_score: train 0.978 valid 0.839 rawah:auroc: train 0.763 valid 0.732 rawah:auprc: train 0.519 valid 0.448 rawah:recall: train 0.978 valid 0.839 rawah:precision: train 0.978 valid 0.841 rawah:accuracy: train 0.981 valid 0.859 rawah:loss: train 0.067 valid 0.845 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.70677
auroc 0.83606
auprc 0.52125
recall 0.69312
precision 0.75332
confusion_matrix
    0     1     2     3     4     5     6     7
    1 62185 15829   597   160  2026   947  4867
    2 21173 76178  5413   701 12738  6638  1312
    3    90   160 11865    48   312   960     2
    4     0     0     0     0     0     0     0
    5    43   194    89     0  4040    41     1
    6    18   170   632    26   154  5664     0
    7   418    29     1     2     8     6 11278
accuracy 0.82359
loss 1.47308

neota:
f1_score 0.71910
auroc 0.62022
auprc 0.31431
recall 0.69090
precision 0.76032
confusion_matrix
    0     1     2     3     4     5     6     7
    1 13069  2660   339   173   793   344  1036
    2  2769  5201   238   127   264    81   239
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7    12     4     2     0     0     2  2032
accuracy 0.76104
loss 1.60737

poudre:
f1_score 0.78924
auroc 0.63046
auprc 0.24862
recall 0.77224
precision 0.82509
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   113  1023  1290    21    10   448   101
    3   534   588 15746   857   420  1893   119
    4     3     4     9   556     0    15     0
    5     0     0     0     0     0     0     0
    6    96   156   411   182    50  7613    35
    7     0     0     0     0     0     0     0
accuracy 0.73995
loss 1.27049

rawah:
f1_score 0.76083
auroc 0.71596
auprc 0.34119
recall 0.74456
precision 0.79148
confusion_matrix
     0      1      2      3      4      5      6      7
     1  75673  19180    156     67   1687    299   7593
     2  25278 108723    187    108   9574    370    823
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     15    120      0      1   2788      1      0
     6      0      0      0      0      0      0      0
     7    203     32      2      0      0      3   4316
accuracy 0.84326
loss 1.24052

saving test results to cover/log/89/seed_11//120812/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 12...(5/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_12//121143/20230409_federatedsame_yspace_cover_split_by_wilderness_model_12/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.987 valid 0.826 comanche:auroc: train 0.852 valid 0.805 comanche:auprc: train 0.597 valid 0.501 comanche:recall: train 0.987 valid 0.826 comanche:precision: train 0.987 valid 0.829 comanche:accuracy: train 0.986 valid 0.810 comanche:loss: train 0.049 valid 0.924 neota:f1_score: train 0.995 valid 0.849 neota:auroc: train 0.667 valid 0.621 neota:auprc: train 0.337 valid 0.287 neota:recall: train 0.995 valid 0.832 neota:precision: train 0.995 valid 0.867 neota:accuracy: train 0.991 valid 0.725 neota:loss: train 0.006 valid 0.997 poudre:f1_score: train 0.998 valid 0.904 poudre:auroc: train 0.727 valid 0.679 poudre:auprc: train 0.479 valid 0.352 poudre:recall: train 0.998 valid 0.903 poudre:precision: train 0.998 valid 0.906 poudre:accuracy: train 0.998 valid 0.751 poudre:loss: train 0.010 valid 0.493 rawah:f1_score: train 0.996 valid 0.829 rawah:auroc: train 0.722 valid 0.686 rawah:auprc: train 0.448 valid 0.396 rawah:recall: train 0.996 valid 0.828 rawah:precision: train 0.996 valid 0.832 rawah:accuracy: train 0.996 valid 0.848 rawah:loss: train 0.022 valid 1.061 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.70347
auroc 0.82199
auprc 0.45856
recall 0.69050
precision 0.75357
confusion_matrix
    0     1     2     3     4     5     6     7
    1 61407 14770  1407   235  2134  1276  5382
    2 20939 76018  6292   720 10645  7413  2126
    3    20   169 11990    46   223   957    32
    4     0     0     0     0     0     0     0
    5    36   203   104     0  4009    56     0
    6    13   116   632    21   116  5752    14
    7   285    13    21     2    14    18 11389
accuracy 0.82603
loss 1.74472

neota:
f1_score 0.71961
auroc 0.60668
auprc 0.26864
recall 0.68331
precision 0.76572
confusion_matrix
    0     1     2     3     4     5     6     7
    1 12913  3111   700   165   542   434   549
    2  2514  5167   573   139   246   184    96
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7    19    18     4     8     1     3  1999
accuracy 0.75159
loss 2.01252

poudre:
f1_score 0.79140
auroc 0.62715
auprc 0.25782
recall 0.77441
precision 0.82290
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   101  1070  1285    40     4   422    84
    3   370   782 16047   886   203  1716   153
    4     1     2     9   561     0    14     0
    5     0     0     0     0     0     0     0
    6   221   128   471   271    18  7330   104
    7     0     0     0     0     0     0     0
accuracy 0.74144
loss 1.62357

rawah:
f1_score 0.76198
auroc 0.69997
auprc 0.33662
recall 0.74611
precision 0.79152
confusion_matrix
     0      1      2      3      4      5      6      7
     1  75836  18672    402     72   1112    697   7864
     2  26086 109024    419    188   7328    883   1135
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     18    130      0      0   2772      5      0
     6      0      0      0      0      0      0      0
     7    212     30     22      6      0     19   4267
accuracy 0.84011
loss 1.48340

saving test results to cover/log/89/seed_12//121143/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 13...(6/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_13//121513/20230409_federatedsame_yspace_cover_split_by_wilderness_model_13/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.975 valid 0.817 comanche:auroc: train 0.888 valid 0.849 comanche:auprc: train 0.680 valid 0.588 comanche:recall: train 0.975 valid 0.820 comanche:precision: train 0.975 valid 0.816 comanche:accuracy: train 0.974 valid 0.803 comanche:loss: train 0.076 valid 1.028 neota:f1_score: train 0.959 valid 0.826 neota:auroc: train 0.665 valid 0.650 neota:auprc: train 0.330 valid 0.288 neota:recall: train 0.958 valid 0.823 neota:precision: train 0.960 valid 0.834 neota:accuracy: train 0.948 valid 0.724 neota:loss: train 0.274 valid 0.968 poudre:f1_score: train 0.983 valid 0.893 poudre:auroc: train 0.748 valid 0.717 poudre:auprc: train 0.495 valid 0.378 poudre:recall: train 0.983 valid 0.895 poudre:precision: train 0.983 valid 0.895 poudre:accuracy: train 0.950 valid 0.706 poudre:loss: train 0.055 valid 0.484 rawah:f1_score: train 0.973 valid 0.823 rawah:auroc: train 0.759 valid 0.735 rawah:auprc: train 0.500 valid 0.457 rawah:recall: train 0.973 valid 0.823 rawah:precision: train 0.974 valid 0.824 rawah:accuracy: train 0.973 valid 0.842 rawah:loss: train 0.095 valid 0.855 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.67657
auroc 0.82637
auprc 0.48772
recall 0.66606
precision 0.73125
confusion_matrix
    0     1     2     3     4     5     6     7
    1 61439 14478  1162    51  1911  1622  5948
    2 25899 70366  6753   382 10622  8091  2040
    3    18   230 11794    14   210  1165     6
    4     0     0     0     0     0     0     0
    5    70   234   109     1  3944    49     1
    6    39   231   519     8   147  5718     2
    7   428    16     5     0     3    24 11266
accuracy 0.81102
loss 1.63411

neota:
f1_score 0.72030
auroc 0.61372
auprc 0.29113
recall 0.69770
precision 0.74914
confusion_matrix
    0     1     2     3     4     5     6     7
    1 13479  2898   299   230   521   304   683
    2  3120  5067   138   132   171   144   147
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7    49    14     5     4     1    23  1956
accuracy 0.75111
loss 1.43422

poudre:
f1_score 0.76141
auroc 0.62204
auprc 0.25049
recall 0.72480
precision 0.82170
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   325  1174   762    42    23   453   227
    3  1087  1062 14457   630   298  2261   362
    4    18     7    16   515     1    23     7
    5     0     0     0     0     0     0     0
    6   379   173   389   193    48  7260   101
    7     0     0     0     0     0     0     0
accuracy 0.70873
loss 1.35677

rawah:
f1_score 0.76587
auroc 0.71205
auprc 0.32850
recall 0.75043
precision 0.79620
confusion_matrix
     0      1      2      3      4      5      6      7
     1  77663  17546     68     45   1041    394   7898
     2  26328 108271    332    156   7854    673   1449
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     18    146      0      0   2759      2      0
     6      0      0      0      0      0      0      0
     7    216     23      0      0      0      1   4316
accuracy 0.84476
loss 1.16802

saving test results to cover/log/89/seed_13//121513/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 14...(7/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_14//121843/20230409_federatedsame_yspace_cover_split_by_wilderness_model_14/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.956 valid 0.815 comanche:auroc: train 0.863 valid 0.823 comanche:auprc: train 0.612 valid 0.532 comanche:recall: train 0.956 valid 0.817 comanche:precision: train 0.956 valid 0.815 comanche:accuracy: train 0.952 valid 0.800 comanche:loss: train 0.127 valid 0.799 neota:f1_score: train 0.990 valid 0.880 neota:auroc: train 0.672 valid 0.627 neota:auprc: train 0.347 valid 0.295 neota:recall: train 0.989 valid 0.876 neota:precision: train 0.990 valid 0.887 neota:accuracy: train 0.991 valid 0.794 neota:loss: train 0.038 valid 0.527 poudre:f1_score: train 0.969 valid 0.897 poudre:auroc: train 0.699 valid 0.633 poudre:auprc: train 0.338 valid 0.240 poudre:recall: train 0.966 valid 0.898 poudre:precision: train 0.973 valid 0.896 poudre:accuracy: train 0.904 valid 0.707 poudre:loss: train 0.115 valid 0.466 rawah:f1_score: train 0.965 valid 0.803 rawah:auroc: train 0.754 valid 0.730 rawah:auprc: train 0.496 valid 0.436 rawah:recall: train 0.965 valid 0.805 rawah:precision: train 0.966 valid 0.803 rawah:accuracy: train 0.967 valid 0.827 rawah:loss: train 0.105 valid 0.959 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.68499
auroc 0.79163
auprc 0.39176
recall 0.67498
precision 0.73834
confusion_matrix
    0     1     2     3     4     5     6     7
    1 63252 14335   578   342  1739   969  5396
    2 24619 70591  9126  1134  9294  7947  1442
    3     9   159 12299    32   197   741     0
    4     0     0     0     0     0     0     0
    5    77   308   166    18  3786    52     1
    6    31   214   801    50   111  5455     2
    7   305    44    13     2     3    29 11346
accuracy 0.80966
loss 1.39556

neota:
f1_score 0.67719
auroc 0.60840
auprc 0.27296
recall 0.60248
precision 0.78894
confusion_matrix
    0     1     2     3     4     5     6     7
    1 10871  2592  1185  1139   795  1083   749
    2  1648  5028   981   450   327   342   143
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7    14    62    13    72     3    83  1805
accuracy 0.67791
loss 1.83668

poudre:
f1_score 0.80810
auroc 0.63830
auprc 0.24805
recall 0.80590
precision 0.82417
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2    12  1138  1260    13     2   527    54
    3    91   465 16788   454   135  2161    63
    4     2     0    43   501     0    41     0
    5     0     0     0     0     0     0     0
    6    45   179   560   136    10  7598    15
    7     0     0     0     0     0     0     0
accuracy 0.73858
loss 0.84943

rawah:
f1_score 0.74171
auroc 0.68900
auprc 0.28142
recall 0.71954
precision 0.78110
confusion_matrix
     0      1      2      3      4      5      6      7
     1  72072  19123    953    178   1517    626  10186
     2  26577 105926    809    362   9128   1015   1246
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     18    108     12      1   2776      9      1
     6      0      0      0      0      0      0      0
     7    188     25     31      3      0     17   4292
accuracy 0.82750
loss 1.17481

saving test results to cover/log/89/seed_14//121843/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 15...(8/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_15//122214/20230409_federatedsame_yspace_cover_split_by_wilderness_model_15/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.781 valid 0.755 comanche:auroc: train 0.604 valid 0.631 comanche:auprc: train 0.267 valid 0.289 comanche:recall: train 0.780 valid 0.758 comanche:precision: train 0.784 valid 0.754 comanche:accuracy: train 0.764 valid 0.737 comanche:loss: train 0.626 valid 0.819 neota:f1_score: train 0.855 valid 0.836 neota:auroc: train 0.525 valid 0.532 neota:auprc: train 0.188 valid 0.192 neota:recall: train 0.818 valid 0.832 neota:precision: train 0.895 valid 0.853 neota:accuracy: train 0.769 valid 0.767 neota:loss: train 0.472 valid 0.781 poudre:f1_score: train 0.975 valid 0.907 poudre:auroc: train 0.633 valid 0.612 poudre:auprc: train 0.322 valid 0.211 poudre:recall: train 0.975 valid 0.909 poudre:precision: train 0.976 valid 0.910 poudre:accuracy: train 0.980 valid 0.718 poudre:loss: train 0.076 valid 0.454 rawah:f1_score: train 0.986 valid 0.834 rawah:auroc: train 0.633 valid 0.646 rawah:auprc: train 0.295 valid 0.298 rawah:recall: train 0.986 valid 0.835 rawah:precision: train 0.986 valid 0.834 rawah:accuracy: train 0.987 valid 0.856 rawah:loss: train 0.040 valid 1.325 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.50393
auroc 0.52245
auprc 0.18038
recall 0.46828
precision 0.62870
confusion_matrix
    0     1     2     3     4     5     6     7
    1 40301 13527  5965   816  2035 14818  9149
    2 26173 48922 16081  3351  8141 18755  2730
    3   177   637 10464  1046   250   861     2
    4     0     0     0     0     0     0     0
    5   234   918   304   240  2569   136     7
    6   247  1040  1306   490   373  3207     1
    7   938   208   148     1     4   233 10210
accuracy 0.59528
loss 1.80288

neota:
f1_score 0.47569
auroc 0.53959
auprc 0.17791
recall 0.39265
precision 0.65822
confusion_matrix
   0    1    2    3    4    5    6    7
   1 6361 4299 1996 1645 1620 1618  875
   2 1838 3698 1181  938  619  543  102
   3    0    0    0    0    0    0    0
   4    0    0    0    0    0    0    0
   5    0    0    0    0    0    0    0
   6    0    0    0    0    0    0    0
   7  151   90    9   44    5  274 1479
accuracy 0.49361
loss 6.32248

poudre:
f1_score 0.78212
auroc 0.55871
auprc 0.20756
recall 0.75459
precision 0.82309
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   435   863   928    26    10   252   492
    3   691  1024 15624   486    88  1934   310
    4     6    10    19   525     0    25     2
    5     0     0     0     0     0     0     0
    6   232   317   382   136    15  7356   105
    7     0     0     0     0     0     0     0
accuracy 0.70441
loss 0.92215

rawah:
f1_score 0.75828
auroc 0.56806
auprc 0.17152
recall 0.73846
precision 0.79326
confusion_matrix
     0      1      2      3      4      5      6      7
     1  73522  19032    568    182   1523   1043   8785
     2  24161 109304    616    148   8819    944   1071
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     10    158      3      0   2750      4      0
     6      0      0      0      0      0      0      0
     7    149     26     14      0      0     11   4356
accuracy 0.83807
loss 1.76501

saving test results to cover/log/89/seed_15//122214/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 16...(9/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_16//122543/20230409_federatedsame_yspace_cover_split_by_wilderness_model_16/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.959 valid 0.818 comanche:auroc: train 0.856 valid 0.832 comanche:auprc: train 0.588 valid 0.537 comanche:recall: train 0.958 valid 0.823 comanche:precision: train 0.959 valid 0.817 comanche:accuracy: train 0.955 valid 0.804 comanche:loss: train 0.115 valid 0.810 neota:f1_score: train 0.999 valid 0.822 neota:auroc: train 0.644 valid 0.629 neota:auprc: train 0.272 valid 0.249 neota:recall: train 0.997 valid 0.814 neota:precision: train 1.000 valid 0.835 neota:accuracy: train 0.998 valid 0.720 neota:loss: train 0.011 valid 1.266 poudre:f1_score: train 0.986 valid 0.903 poudre:auroc: train 0.736 valid 0.701 poudre:auprc: train 0.493 valid 0.377 poudre:recall: train 0.985 valid 0.904 poudre:precision: train 0.987 valid 0.905 poudre:accuracy: train 0.988 valid 0.716 poudre:loss: train 0.058 valid 0.548 rawah:f1_score: train 0.977 valid 0.816 rawah:auroc: train 0.760 valid 0.722 rawah:auprc: train 0.504 valid 0.435 rawah:recall: train 0.976 valid 0.817 rawah:precision: train 0.977 valid 0.820 rawah:accuracy: train 0.979 valid 0.838 rawah:loss: train 0.082 valid 1.252 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.63007
auroc 0.80349
auprc 0.41912
recall 0.61194
precision 0.71150
confusion_matrix
    0     1     2     3     4     5     6     7
    1 56429 13844  2407   275  2350  5094  6212
    2 25570 62678 10470  1269 11591 11074  1501
    3    39   136 11689    88   237  1241     7
    4     0     0     0     0     0     0     0
    5    99   315   124     3  3716   145     6
    6    35    76   752    40   110  5649     2
    7   319    39    97     0     5   284 10998
accuracy 0.77560
loss 1.74041

neota:
f1_score 0.69007
auroc 0.60318
auprc 0.26568
recall 0.64267
precision 0.75017
confusion_matrix
    0     1     2     3     4     5     6     7
    1 12278  3055   708   199   616   809   749
    2  2586  4944   549    74   271   368   127
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7    20     4    83    25     8   249  1663
accuracy 0.67718
loss 1.99766

poudre:
f1_score 0.79102
auroc 0.62523
auprc 0.27678
recall 0.77323
precision 0.82507
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   128   940  1203     5    55   460   215
    3   458   655 15954   562   348  1899   281
    4    13     3    20   528     6    15     2
    5     0     0     0     0     0     0     0
    6   137   122   440   155    41  7548   100
    7     0     0     0     0     0     0     0
accuracy 0.72180
loss 1.27086

rawah:
f1_score 0.75232
auroc 0.71532
auprc 0.34104
recall 0.73195
precision 0.78902
confusion_matrix
     0      1      2      3      4      5      6      7
     1  74212  18631    445     28   1790    858   8691
     2  25684 106984    410     31  10213    840    901
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     10     84      0      1   2826      4      0
     6      0      0      0      0      0      0      0
     7    253     27      5      0      0     37   4234
accuracy 0.83552
loss 1.44634

saving test results to cover/log/89/seed_16//122543/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 17...(10/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_17//122914/20230409_federatedsame_yspace_cover_split_by_wilderness_model_17/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.979 valid 0.832 comanche:auroc: train 0.889 valid 0.851 comanche:auprc: train 0.678 valid 0.601 comanche:recall: train 0.979 valid 0.835 comanche:precision: train 0.979 valid 0.831 comanche:accuracy: train 0.977 valid 0.817 comanche:loss: train 0.063 valid 0.929 neota:f1_score: train 0.999 valid 0.886 neota:auroc: train 0.659 valid 0.618 neota:auprc: train 0.321 valid 0.254 neota:recall: train 0.997 valid 0.894 neota:precision: train 1.000 valid 0.888 neota:accuracy: train 0.998 valid 0.761 neota:loss: train 0.013 valid 0.777 poudre:f1_score: train 0.993 valid 0.903 poudre:auroc: train 0.754 valid 0.720 poudre:auprc: train 0.516 valid 0.400 poudre:recall: train 0.993 valid 0.903 poudre:precision: train 0.994 valid 0.905 poudre:accuracy: train 0.994 valid 0.754 poudre:loss: train 0.026 valid 0.444 rawah:f1_score: train 0.983 valid 0.831 rawah:auroc: train 0.763 valid 0.737 rawah:auprc: train 0.518 valid 0.464 rawah:recall: train 0.983 valid 0.833 rawah:precision: train 0.984 valid 0.834 rawah:accuracy: train 0.983 valid 0.852 rawah:loss: train 0.055 valid 0.959 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.69587
auroc 0.83437
auprc 0.49943
recall 0.68213
precision 0.74881
confusion_matrix
    0     1     2     3     4     5     6     7
    1 61302 15016   886   131  1911  2512  4853
    2 21960 74059  6255   879 10187  9338  1475
    3    18   136 11804    51   205  1204    19
    4     0     0     0     0     0     0     0
    5    58   186   145    11  3935    68     5
    6    10   108   468     4    70  6003     1
    7   278    44     2     1     3    20 11394
accuracy 0.82444
loss 1.56622

neota:
f1_score 0.71076
auroc 0.61560
auprc 0.27155
recall 0.67483
precision 0.76143
confusion_matrix
    0     1     2     3     4     5     6     7
    1 12582  3407   355   228   546   571   725
    2  2266  5228   389   304   320   197   215
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7    27     2     1     1     0     1  2020
accuracy 0.75128
loss 1.69033

poudre:
f1_score 0.73066
auroc 0.65513
auprc 0.27146
recall 0.69668
precision 0.79622
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   297  1033  1040    22    38   509    67
    3  1269  1492 13559   886   207  2488   256
    4    13     5     7   547     0    15     0
    5     0     0     0     0     0     0     0
    6   327   287   283   221    18  7359    48
    7     0     0     0     0     0     0     0
accuracy 0.70239
loss 1.68224

rawah:
f1_score 0.75897
auroc 0.71415
auprc 0.34405
recall 0.74316
precision 0.78931
confusion_matrix
     0      1      2      3      4      5      6      7
     1  75754  18468    242    158   1020    352   8661
     2  26834 108304    278    212   7593    853    989
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     30    118      1      1   2769      6      0
     6      0      0      0      0      0      0      0
     7    197     34      0      6      0      7   4312
accuracy 0.84089
loss 1.39354

saving test results to cover/log/89/seed_17//122914/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 18...(11/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_18//123249/20230409_federatedsame_yspace_cover_split_by_wilderness_model_18/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.989 valid 0.828 comanche:auroc: train 0.872 valid 0.822 comanche:auprc: train 0.629 valid 0.554 comanche:recall: train 0.989 valid 0.831 comanche:precision: train 0.990 valid 0.826 comanche:accuracy: train 0.989 valid 0.812 comanche:loss: train 0.040 valid 0.895 neota:f1_score: train 0.997 valid 0.861 neota:auroc: train 0.627 valid 0.578 neota:auprc: train 0.269 valid 0.238 neota:recall: train 0.997 valid 0.850 neota:precision: train 0.997 valid 0.874 neota:accuracy: train 0.998 valid 0.754 neota:loss: train 0.009 valid 0.781 poudre:f1_score: train 0.997 valid 0.908 poudre:auroc: train 0.767 valid 0.721 poudre:auprc: train 0.535 valid 0.383 poudre:recall: train 0.997 valid 0.907 poudre:precision: train 0.997 valid 0.912 poudre:accuracy: train 0.997 valid 0.754 poudre:loss: train 0.011 valid 0.478 rawah:f1_score: train 0.997 valid 0.829 rawah:auroc: train 0.765 valid 0.727 rawah:auprc: train 0.507 valid 0.439 rawah:recall: train 0.997 valid 0.830 rawah:precision: train 0.997 valid 0.831 rawah:accuracy: train 0.997 valid 0.851 rawah:loss: train 0.009 valid 1.281 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.68860
auroc 0.83490
auprc 0.53041
recall 0.67321
precision 0.74734
confusion_matrix
    0     1     2     3     4     5     6     7
    1 60754 14973  1048   126  2473  1613  5624
    2 20786 72293  7320   985 12403  8826  1540
    3    29   122 11959    44   289   981    13
    4     0     0     0     0     0     0     0
    5    35   166    92    10  4032    73     0
    6    48   120   474    33    99  5890     0
    7   323    18     0     0     4    32 11365
accuracy 0.82337
loss 1.96289

neota:
f1_score 0.74662
auroc 0.62473
auprc 0.30622
recall 0.72095
precision 0.78145
confusion_matrix
    0     1     2     3     4     5     6     7
    1 13700  2699   395   122   357   313   828
    2  2451  5447   348   166   268   100   139
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7    11     2     0     0     0     1  2038
accuracy 0.78263
loss 1.68863

poudre:
f1_score 0.78316
auroc 0.68799
auprc 0.30219
recall 0.76964
precision 0.81668
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   144   971  1265    48    18   465    95
    3   310   592 15864   893   246  2175    77
    4     1     1     3   569     0    12     1
    5     0     0     0     0     0     0     0
    6   170   133   476   227    28  7450    59
    7     0     0     0     0     0     0     0
accuracy 0.73786
loss 1.43662

rawah:
f1_score 0.76414
auroc 0.72330
auprc 0.36459
recall 0.74801
precision 0.79499
confusion_matrix
     0      1      2      3      4      5      6      7
     1  75956  17777    260     38   1398    261   8965
     2  25928 109398    229     35   8024    494    955
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     21    147      0      0   2754      3      0
     6      0      0      0      0      0      0      0
     7    221     42      7      3      0      4   4279
accuracy 0.84016
loss 1.65744

saving test results to cover/log/89/seed_18//123249/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 19...(12/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_19//123623/20230409_federatedsame_yspace_cover_split_by_wilderness_model_19/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.974 valid 0.824 comanche:auroc: train 0.739 valid 0.732 comanche:auprc: train 0.437 valid 0.399 comanche:recall: train 0.974 valid 0.829 comanche:precision: train 0.974 valid 0.825 comanche:accuracy: train 0.972 valid 0.809 comanche:loss: train 0.074 valid 1.057 neota:f1_score: train 0.980 valid 0.860 neota:auroc: train 0.603 valid 0.580 neota:auprc: train 0.247 valid 0.217 neota:recall: train 0.968 valid 0.858 neota:precision: train 0.992 valid 0.863 neota:accuracy: train 0.946 valid 0.744 neota:loss: train 0.127 valid 0.724 poudre:f1_score: train 0.993 valid 0.894 poudre:auroc: train 0.697 valid 0.665 poudre:auprc: train 0.424 valid 0.309 poudre:recall: train 0.993 valid 0.893 poudre:precision: train 0.993 valid 0.897 poudre:accuracy: train 0.994 valid 0.704 poudre:loss: train 0.031 valid 0.725 rawah:f1_score: train 0.990 valid 0.832 rawah:auroc: train 0.685 valid 0.678 rawah:auprc: train 0.392 valid 0.357 rawah:recall: train 0.989 valid 0.834 rawah:precision: train 0.990 valid 0.834 rawah:accuracy: train 0.991 valid 0.856 rawah:loss: train 0.037 valid 1.070 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.69468
auroc 0.62122
auprc 0.31392
recall 0.67759
precision 0.75406
confusion_matrix
    0     1     2     3     4     5     6     7
    1 61462 14396   740   343  2438  2705  4527
    2 21149 72704  5964  1183 12585  9010  1558
    3    53   122 11875    55   318   995    19
    4     0     0     0     0     0     0     0
    5    48   155    76     8  4085    34     2
    6    35   123   462    68   151  5819     6
    7   227    43     4     0     6    32 11430
accuracy 0.82539
loss 1.74722

neota:
f1_score 0.59792
auroc 0.57819
auprc 0.22626
recall 0.52010
precision 0.73221
confusion_matrix
   0    1    2    3    4    5    6    7
   1 9179 3828 1056  604 1716 1314  717
   2 1650 4355  638  465 1030  573  208
   3    0    0    0    0    0    0    0
   4    0    0    0    0    0    0    0
   5    0    0    0    0    0    0    0
   6    0    0    0    0    0    0    0
   7  123   10   12   63    7   88 1749
accuracy 0.61303
loss 2.61976

poudre:
f1_score 0.75165
auroc 0.60997
auprc 0.23811
recall 0.71437
precision 0.81896
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   248  1323   848    47    72   370    98
    3  1061  1228 13845  1015   432  2279   297
    4     6     8     5   554     1    11     2
    5     0     0     0     0     0     0     0
    6   303   177   350   239    42  7347    85
    7     0     0     0     0     0     0     0
accuracy 0.73269
loss 1.55549

rawah:
f1_score 0.74452
auroc 0.61767
auprc 0.23109
recall 0.72030
precision 0.78533
confusion_matrix
     0      1      2      3      4      5      6      7
     1  71221  21095    314    304   2326    850   8545
     2  22784 107025    776    362  11002   2144    970
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     13    111      1      2   2794      4      0
     6      0      0      0      0      0      0      0
     7    255     63      4      8      0      5   4221
accuracy 0.82500
loss 1.65430

saving test results to cover/log/89/seed_19//123623/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 20...(13/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_20//123955/20230409_federatedsame_yspace_cover_split_by_wilderness_model_20/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.997 valid 0.814 comanche:auroc: train 0.851 valid 0.796 comanche:auprc: train 0.609 valid 0.511 comanche:recall: train 0.996 valid 0.817 comanche:precision: train 0.997 valid 0.815 comanche:accuracy: train 0.996 valid 0.797 comanche:loss: train 0.016 valid 1.257 neota:f1_score: train 0.997 valid 0.860 neota:auroc: train 0.617 valid 0.581 neota:auprc: train 0.263 valid 0.219 neota:recall: train 0.997 valid 0.832 neota:precision: train 0.997 valid 0.895 neota:accuracy: train 0.993 valid 0.738 neota:loss: train 0.005 valid 0.736 poudre:f1_score: train 0.996 valid 0.915 poudre:auroc: train 0.680 valid 0.643 poudre:auprc: train 0.410 valid 0.265 poudre:recall: train 0.996 valid 0.912 poudre:precision: train 0.996 valid 0.920 poudre:accuracy: train 0.997 valid 0.721 poudre:loss: train 0.014 valid 0.504 rawah:f1_score: train 0.999 valid 0.826 rawah:auroc: train 0.758 valid 0.721 rawah:auprc: train 0.516 valid 0.444 rawah:recall: train 0.999 valid 0.826 rawah:precision: train 0.999 valid 0.828 rawah:accuracy: train 0.998 valid 0.851 rawah:loss: train 0.005 valid 1.161 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.68770
auroc 0.79347
auprc 0.41001
recall 0.67378
precision 0.74527
confusion_matrix
    0     1     2     3     4     5     6     7
    1 61084 14066  1578   192  2468  2065  5158
    2 22805 72297  7133   701 10770  8888  1559
    3    18   211 11803    27   315  1055     8
    4     0     0     0     0     0     0     0
    5    36   218    93     6  4013    42     0
    6    41   117   532    16   123  5829     6
    7   236    47    16     0     6    28 11409
accuracy 0.82045
loss 2.21895

neota:
f1_score 0.69466
auroc 0.59245
auprc 0.23620
recall 0.64165
precision 0.76717
confusion_matrix
    0     1     2     3     4     5     6     7
    1 12231  2661   879   325   795   776   747
    2  2285  4670   802   207   419   352   184
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7    35     8    26     5    19     5  1954
accuracy 0.71336
loss 2.47153

poudre:
f1_score 0.79371
auroc 0.59384
auprc 0.25500
recall 0.77747
precision 0.83164
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   217   870  1093    59    55   461   251
    3   398   484 16163   767   230  1913   202
    4     3     1     7   563     0    12     1
    5     0     0     0     0     0     0     0
    6   115   103   471   202    15  7511   126
    7     0     0     0     0     0     0     0
accuracy 0.73240
loss 1.60145

rawah:
f1_score 0.75457
auroc 0.70297
auprc 0.29231
recall 0.73498
precision 0.78984
confusion_matrix
     0      1      2      3      4      5      6      7
     1  74907  18551    483     18   1532    811   8353
     2  25856 107071    778     49   9148   1176    985
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     25    119      2      0   2770      8      1
     6      0      0      0      0      0      0      0
     7    214     32     14      2      0      5   4289
accuracy 0.83556
loss 1.90443

saving test results to cover/log/89/seed_20//123955/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 21...(14/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_21//124331/20230409_federatedsame_yspace_cover_split_by_wilderness_model_21/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.996 valid 0.835 comanche:auroc: train 0.876 valid 0.825 comanche:auprc: train 0.648 valid 0.541 comanche:recall: train 0.996 valid 0.838 comanche:precision: train 0.996 valid 0.835 comanche:accuracy: train 0.996 valid 0.820 comanche:loss: train 0.017 valid 1.053 neota:f1_score: train 0.997 valid 0.818 neota:auroc: train 0.666 valid 0.615 neota:auprc: train 0.331 valid 0.255 neota:recall: train 0.997 valid 0.796 neota:precision: train 0.997 valid 0.850 neota:accuracy: train 0.994 valid 0.721 neota:loss: train 0.005 valid 0.952 poudre:f1_score: train 0.997 valid 0.913 poudre:auroc: train 0.745 valid 0.669 poudre:auprc: train 0.472 valid 0.358 poudre:recall: train 0.997 valid 0.909 poudre:precision: train 0.997 valid 0.918 poudre:accuracy: train 0.998 valid 0.759 poudre:loss: train 0.009 valid 0.480 rawah:f1_score: train 0.997 valid 0.804 rawah:auroc: train 0.755 valid 0.715 rawah:auprc: train 0.504 valid 0.433 rawah:recall: train 0.997 valid 0.805 rawah:precision: train 0.997 valid 0.809 rawah:accuracy: train 0.998 valid 0.833 rawah:loss: train 0.007 valid 1.258 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.68969
auroc 0.82831
auprc 0.49868
recall 0.67432
precision 0.75067
confusion_matrix
    0     1     2     3     4     5     6     7
    1 61242 13697  1392   266  2395  2475  5144
    2 21698 72007  7576   973 11839  8432  1628
    3    27   158 11990    45   255   960     2
    4     0     0     0     0     0     0     0
    5    25   164    87     8  4087    37     0
    6    10   160   535    57   151  5751     0
    7   207    25     5     1     7     7 11490
accuracy 0.82468
loss 2.73083

neota:
f1_score 0.71737
auroc 0.62284
auprc 0.31224
recall 0.67882
precision 0.76938
confusion_matrix
    0     1     2     3     4     5     6     7
    1 12598  2919   398   249   962   540   748
    2  2411  5360   285   247   300   175   141
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7    31    12     0    14     2     4  1989
accuracy 0.75147
loss 2.17933

poudre:
f1_score 0.79034
auroc 0.67965
auprc 0.31731
recall 0.77249
precision 0.82565
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   129  1032  1242    41    50   372   140
    3   607   698 15882   969   142  1793    66
    4     3     2     8   567     0     7     0
    5     0     0     0     0     0     0     0
    6   150   104   482   265    18  7465    59
    7     0     0     0     0     0     0     0
accuracy 0.74274
loss 1.56489

rawah:
f1_score 0.75852
auroc 0.71063
auprc 0.32885
recall 0.74031
precision 0.79124
confusion_matrix
     0      1      2      3      4      5      6      7
     1  75689  18808    435    129   1293    637   7664
     2  25548 107721    567    474   8400   1197   1156
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     19    134      4      2   2750     16      0
     6      0      0      0      0      0      0      0
     7    237     39     11      4      0     18   4247
accuracy 0.83454
loss 1.76597

saving test results to cover/log/89/seed_21//124331/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 22...(15/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_22//124701/20230409_federatedsame_yspace_cover_split_by_wilderness_model_22/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.991 valid 0.827 comanche:auroc: train 0.883 valid 0.828 comanche:auprc: train 0.681 valid 0.580 comanche:recall: train 0.991 valid 0.831 comanche:precision: train 0.991 valid 0.827 comanche:accuracy: train 0.991 valid 0.811 comanche:loss: train 0.034 valid 1.011 neota:f1_score: train 0.997 valid 0.860 neota:auroc: train 0.670 valid 0.602 neota:auprc: train 0.340 valid 0.277 neota:recall: train 0.997 valid 0.850 neota:precision: train 0.997 valid 0.876 neota:accuracy: train 0.994 valid 0.736 neota:loss: train 0.005 valid 0.853 poudre:f1_score: train 0.997 valid 0.897 poudre:auroc: train 0.752 valid 0.702 poudre:auprc: train 0.513 valid 0.367 poudre:recall: train 0.997 valid 0.898 poudre:precision: train 0.997 valid 0.898 poudre:accuracy: train 0.998 valid 0.705 poudre:loss: train 0.010 valid 0.586 rawah:f1_score: train 0.982 valid 0.814 rawah:auroc: train 0.761 valid 0.711 rawah:auprc: train 0.510 valid 0.437 rawah:recall: train 0.982 valid 0.815 rawah:precision: train 0.982 valid 0.816 rawah:accuracy: train 0.985 valid 0.837 rawah:loss: train 0.053 valid 1.128 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.68721
auroc 0.84305
auprc 0.54955
recall 0.67531
precision 0.74572
confusion_matrix
    0     1     2     3     4     5     6     7
    1 63970 13196  1008   193  2388  1118  4738
    2 25308 69800  7766  1264 12042  6359  1614
    3    42   111 12047    56   287   892     2
    4     0     0     0     0     0     0     0
    5    72   177   115     8  3993    42     1
    6    45   148   678    23   157  5612     1
    7   285    24    20     0     3    20 11390
accuracy 0.81923
loss 1.80143

neota:
f1_score 0.69560
auroc 0.61825
auprc 0.31480
recall 0.64962
precision 0.75842
confusion_matrix
    0     1     2     3     4     5     6     7
    1 12537  2241   701   483  1047   693   712
    2  2957  4534   537   222   256   257   156
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7    16     2     2    10     1     3  2018
accuracy 0.72421
loss 2.18614

poudre:
f1_score 0.77448
auroc 0.64967
auprc 0.28170
recall 0.75893
precision 0.81357
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2    76   808  1320    38    17   477   270
    3   454   456 15757   906   252  2154   178
    4     2     2    15   555     0    13     0
    5     0     0     0     0     0     0     0
    6   167   140   489   234    42  7388    83
    7     0     0     0     0     0     0     0
accuracy 0.71520
loss 1.50737

rawah:
f1_score 0.75557
auroc 0.72167
auprc 0.38506
recall 0.73746
precision 0.79165
confusion_matrix
     0      1      2      3      4      5      6      7
     1  78027  16692    292     73   1627    299   7645
     2  28415 104556    410     52  10105    669    856
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     17    111      5      0   2787      5      0
     6      0      0      0      0      0      0      0
     7    190     26     26      0      2      8   4304
accuracy 0.84096
loss 1.42661

saving test results to cover/log/89/seed_22//124701/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 23...(16/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_23//125034/20230409_federatedsame_yspace_cover_split_by_wilderness_model_23/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.993 valid 0.837 comanche:auroc: train 0.899 valid 0.849 comanche:auprc: train 0.729 valid 0.613 comanche:recall: train 0.993 valid 0.840 comanche:precision: train 0.993 valid 0.837 comanche:accuracy: train 0.993 valid 0.822 comanche:loss: train 0.026 valid 0.903 neota:f1_score: train 0.997 valid 0.851 neota:auroc: train 0.660 valid 0.572 neota:auprc: train 0.292 valid 0.216 neota:recall: train 0.997 valid 0.823 neota:precision: train 0.997 valid 0.887 neota:accuracy: train 0.994 valid 0.730 neota:loss: train 0.007 valid 1.161 poudre:f1_score: train 0.996 valid 0.908 poudre:auroc: train 0.775 valid 0.715 poudre:auprc: train 0.551 valid 0.405 poudre:recall: train 0.996 valid 0.906 poudre:precision: train 0.997 valid 0.913 poudre:accuracy: train 0.997 valid 0.753 poudre:loss: train 0.015 valid 0.547 rawah:f1_score: train 0.995 valid 0.832 rawah:auroc: train 0.771 valid 0.734 rawah:auprc: train 0.536 valid 0.455 rawah:recall: train 0.995 valid 0.832 rawah:precision: train 0.995 valid 0.835 rawah:accuracy: train 0.996 valid 0.856 rawah:loss: train 0.016 valid 1.148 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.68817
auroc 0.84341
auprc 0.55809
recall 0.67402
precision 0.74340
confusion_matrix
    0     1     2     3     4     5     6     7
    1 61645 15268   716   237  2441   892  5412
    2 22642 71727  6698  1414 12693  7407  1572
    3    23   114 11941   112   233   995    19
    4     0     0     0     0     0     0     0
    5    40   151   102     8  4065    40     2
    6    24   172   628    56   135  5644     5
    7   220    33     1     1    14     2 11471
accuracy 0.82070
loss 1.94910

neota:
f1_score 0.72124
auroc 0.62442
auprc 0.28727
recall 0.68348
precision 0.77292
confusion_matrix
    0     1     2     3     4     5     6     7
    1 12944  2749   541   265   669   446   800
    2  2387  5101   531   230   333   139   198
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7     8     0     1     0     1     3  2039
accuracy 0.75618
loss 1.92584

poudre:
f1_score 0.79251
auroc 0.68996
auprc 0.32624
recall 0.78553
precision 0.82457
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   154   880  1292    47    25   552    56
    3   235   426 16279   891   152  2095    79
    4     0     2    11   564     0    10     0
    5     0     0     0     0     0     0     0
    6    54    61   477   278    14  7644    15
    7     0     0     0     0     0     0     0
accuracy 0.73899
loss 1.46827

rawah:
f1_score 0.75806
auroc 0.72256
auprc 0.38403
recall 0.74022
precision 0.79214
confusion_matrix
     0      1      2      3      4      5      6      7
     1  76033  17718    255     74   1524    417   8634
     2  26629 107226    385    132   8674    683   1334
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     21    100      0      0   2802      1      1
     6      0      0      0      0      0      0      0
     7    171     28     25      3      1      5   4323
accuracy 0.84312
loss 1.56650

saving test results to cover/log/89/seed_23//125034/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 24...(17/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_24//125406/20230409_federatedsame_yspace_cover_split_by_wilderness_model_24/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.994 valid 0.823 comanche:auroc: train 0.899 valid 0.846 comanche:auprc: train 0.732 valid 0.616 comanche:recall: train 0.994 valid 0.825 comanche:precision: train 0.994 valid 0.822 comanche:accuracy: train 0.994 valid 0.806 comanche:loss: train 0.020 valid 1.143 neota:f1_score: train 0.997 valid 0.822 neota:auroc: train 0.670 valid 0.635 neota:auprc: train 0.333 valid 0.293 neota:recall: train 0.997 valid 0.805 neota:precision: train 0.997 valid 0.844 neota:accuracy: train 0.994 valid 0.698 neota:loss: train 0.008 valid 1.080 poudre:f1_score: train 0.997 valid 0.911 poudre:auroc: train 0.758 valid 0.726 poudre:auprc: train 0.504 valid 0.382 poudre:recall: train 0.997 valid 0.909 poudre:precision: train 0.997 valid 0.914 poudre:accuracy: train 0.980 valid 0.756 poudre:loss: train 0.015 valid 0.454 rawah:f1_score: train 0.994 valid 0.819 rawah:auroc: train 0.773 valid 0.729 rawah:auprc: train 0.540 valid 0.464 rawah:recall: train 0.994 valid 0.820 rawah:precision: train 0.994 valid 0.820 rawah:accuracy: train 0.995 valid 0.844 rawah:loss: train 0.018 valid 1.254 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.69445
auroc 0.83981
auprc 0.53105
recall 0.68198
precision 0.74442
confusion_matrix
    0     1     2     3     4     5     6     7
    1 62736 15450   925   154  2050   877  4419
    2 23339 72862  6800  1179 11522  7494   957
    3    45   146 11881   118   278   967     2
    4     0     0     0     0     0     0     0
    5    65   168    97     9  4027    42     0
    6    42   191   565    56   162  5647     1
    7   371    21    15     6     5    18 11306
accuracy 0.81987
loss 1.87359

neota:
f1_score 0.67236
auroc 0.61741
auprc 0.28213
recall 0.62947
precision 0.72890
confusion_matrix
    0     1     2     3     4     5     6     7
    1 12115  3063   666   396   746   759   669
    2  3057  4392   543   263   346   208   110
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7     1    19    16     8     0    18  1990
accuracy 0.70671
loss 2.30345

poudre:
f1_score 0.79658
auroc 0.67239
auprc 0.28322
recall 0.78491
precision 0.82932
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2    86   925  1217    19    14   560   185
    3   294   437 16397   869   212  1813   135
    4     5     1    10   558     1    10     2
    5     0     0     0     0     0     0     0
    6   133    94   535   229    26  7467    59
    7     0     0     0     0     0     0     0
accuracy 0.73646
loss 1.47502

rawah:
f1_score 0.75665
auroc 0.72038
auprc 0.34072
recall 0.74036
precision 0.78782
confusion_matrix
     0      1      2      3      4      5      6      7
     1  75578  18728    371     24   1248    268   8438
     2  26720 107805    313     82   9138    292    713
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     34    139      0      0   2750      2      0
     6      0      0      0      0      0      0      0
     7    204     38      5      0      0     22   4287
accuracy 0.83661
loss 1.73530

saving test results to cover/log/89/seed_24//125406/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 25...(18/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_25//125740/20230409_federatedsame_yspace_cover_split_by_wilderness_model_25/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.989 valid 0.834 comanche:auroc: train 0.808 valid 0.770 comanche:auprc: train 0.553 valid 0.486 comanche:recall: train 0.989 valid 0.837 comanche:precision: train 0.989 valid 0.833 comanche:accuracy: train 0.988 valid 0.819 comanche:loss: train 0.034 valid 0.890 neota:f1_score: train 0.996 valid 0.873 neota:auroc: train 0.630 valid 0.621 neota:auprc: train 0.267 valid 0.264 neota:recall: train 0.995 valid 0.867 neota:precision: train 0.997 valid 0.887 neota:accuracy: train 0.991 valid 0.815 neota:loss: train 0.019 valid 0.762 poudre:f1_score: train 0.996 valid 0.895 poudre:auroc: train 0.722 valid 0.684 poudre:auprc: train 0.457 valid 0.340 poudre:recall: train 0.996 valid 0.891 poudre:precision: train 0.996 valid 0.903 poudre:accuracy: train 0.996 valid 0.739 poudre:loss: train 0.017 valid 0.554 rawah:f1_score: train 0.979 valid 0.827 rawah:auroc: train 0.729 valid 0.702 rawah:auprc: train 0.476 valid 0.417 rawah:recall: train 0.979 valid 0.825 rawah:precision: train 0.979 valid 0.832 rawah:accuracy: train 0.982 valid 0.850 rawah:loss: train 0.076 valid 0.923 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.69260
auroc 0.78931
auprc 0.42824
recall 0.67642
precision 0.75376
confusion_matrix
    0     1     2     3     4     5     6     7
    1 61641 13504  1156   293  2481  2167  5369
    2 21738 72208  6344   943 12268  9228  1424
    3     8   153 11952    55   278   989     2
    4     0     0     0     0     0     0     0
    5    29   162   103     5  4058    51     0
    6    14   154   546    34   124  5792     0
    7   243    31    10     1     7    15 11435
accuracy 0.82440
loss 1.77776

neota:
f1_score 0.69094
auroc 0.60179
auprc 0.26043
recall 0.66020
precision 0.74020
confusion_matrix
    0     1     2     3     4     5     6     7
    1 11943  4026   391   153   746   391   764
    2  2456  5424   362   102   231    95   249
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7     6     9     0     0     1     3  2033
accuracy 0.74915
loss 1.71452

poudre:
f1_score 0.78189
auroc 0.62557
auprc 0.23739
recall 0.75797
precision 0.82808
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   196  1032   993    45    41   474   225
    3   506   633 15497  1109   213  1993   206
    4     1     0     5   569     1    11     0
    5     0     0     0     0     0     0     0
    6   227   139   428   268    14  7379    88
    7     0     0     0     0     0     0     0
accuracy 0.73630
loss 1.43425

rawah:
f1_score 0.76501
auroc 0.69981
auprc 0.32402
recall 0.74652
precision 0.79864
confusion_matrix
     0      1      2      3      4      5      6      7
     1  75497  17849    551    118   1531    608   8501
     2  24636 109376    368     81   8937    857    808
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     18     99      0      1   2806      1      0
     6      0      0      0      0      0      0      0
     7    163     34     24      4      0      7   4324
accuracy 0.84594
loss 1.41752

saving test results to cover/log/89/seed_25//125740/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 26...(19/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_26//130114/20230409_federatedsame_yspace_cover_split_by_wilderness_model_26/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.992 valid 0.845 comanche:auroc: train 0.912 valid 0.865 comanche:auprc: train 0.783 valid 0.657 comanche:recall: train 0.992 valid 0.847 comanche:precision: train 0.992 valid 0.843 comanche:accuracy: train 0.991 valid 0.831 comanche:loss: train 0.028 valid 0.924 neota:f1_score: train 0.992 valid 0.855 neota:auroc: train 0.690 valid 0.643 neota:auprc: train 0.375 valid 0.315 neota:recall: train 0.992 valid 0.850 neota:precision: train 0.992 valid 0.871 neota:accuracy: train 0.984 valid 0.739 neota:loss: train 0.093 valid 0.775 poudre:f1_score: train 0.996 valid 0.900 poudre:auroc: train 0.751 valid 0.695 poudre:auprc: train 0.532 valid 0.371 poudre:recall: train 0.996 valid 0.899 poudre:precision: train 0.996 valid 0.903 poudre:accuracy: train 0.997 valid 0.709 poudre:loss: train 0.012 valid 0.543 rawah:f1_score: train 0.998 valid 0.830 rawah:auroc: train 0.777 valid 0.741 rawah:auprc: train 0.551 valid 0.476 rawah:recall: train 0.998 valid 0.831 rawah:precision: train 0.998 valid 0.832 rawah:accuracy: train 0.998 valid 0.856 rawah:loss: train 0.007 valid 1.125 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.68673
auroc 0.85098
auprc 0.58175
recall 0.67148
precision 0.74898
confusion_matrix
    0     1     2     3     4     5     6     7
    1 61451 13992  1216   258  2803  1439  5452
    2 21900 70883  7769  1181 12850  7995  1575
    3    16    75 12135    70   246   892     3
    4     0     0     0     0     0     0     0
    5    30    97   134     6  4097    43     1
    6    22    99   575    49   103  5816     0
    7   220    20     8     0     7     3 11484
accuracy 0.82729
loss 1.95200

neota:
f1_score 0.70329
auroc 0.63280
auprc 0.32002
recall 0.67259
precision 0.74693
confusion_matrix
    0     1     2     3     4     5     6     7
    1 13230  2731   423   246   490   441   853
    2  2931  4491   355   213   381   298   250
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7     5     0     3     0     0     1  2043
accuracy 0.73921
loss 2.18767

poudre:
f1_score 0.78422
auroc 0.64934
auprc 0.27740
recall 0.77565
precision 0.81524
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   114   868  1319    29     8   638    30
    3   238   521 15966   890   314  2123   105
    4     0     3     4   559     2    19     0
    5     0     0     0     0     0     0     0
    6    55   143   428   205    30  7655    27
    7     0     0     0     0     0     0     0
accuracy 0.73230
loss 1.55429

rawah:
f1_score 0.75192
auroc 0.72197
auprc 0.35956
recall 0.73308
precision 0.78906
confusion_matrix
     0      1      2      3      4      5      6      7
     1  77403  16464    461    102   1327    673   8225
     2  29239 104115    606    134   9091    938    940
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     16     95      1      4   2802      7      0
     6      0      0      0      0      0      0      0
     7    259     34     24      2      2      7   4228
accuracy 0.83582
loss 1.58786

saving test results to cover/log/89/seed_26//130114/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 27...(20/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_27//130445/20230409_federatedsame_yspace_cover_split_by_wilderness_model_27/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.960 valid 0.827 comanche:auroc: train 0.836 valid 0.813 comanche:auprc: train 0.549 valid 0.512 comanche:recall: train 0.960 valid 0.831 comanche:precision: train 0.960 valid 0.826 comanche:accuracy: train 0.957 valid 0.811 comanche:loss: train 0.130 valid 0.750 neota:f1_score: train 0.992 valid 0.853 neota:auroc: train 0.673 valid 0.645 neota:auprc: train 0.339 valid 0.289 neota:recall: train 0.992 valid 0.858 neota:precision: train 0.992 valid 0.850 neota:accuracy: train 0.989 valid 0.760 neota:loss: train 0.030 valid 0.766 poudre:f1_score: train 0.971 valid 0.877 poudre:auroc: train 0.731 valid 0.655 poudre:auprc: train 0.437 valid 0.310 poudre:recall: train 0.971 valid 0.877 poudre:precision: train 0.971 valid 0.880 poudre:accuracy: train 0.975 valid 0.652 poudre:loss: train 0.126 valid 0.583 rawah:f1_score: train 0.987 valid 0.837 rawah:auroc: train 0.739 valid 0.716 rawah:auprc: train 0.471 valid 0.422 rawah:recall: train 0.987 valid 0.838 rawah:precision: train 0.987 valid 0.838 rawah:accuracy: train 0.989 valid 0.857 rawah:loss: train 0.056 valid 0.757 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.65770
auroc 0.80217
auprc 0.39653
recall 0.64712
precision 0.72884
confusion_matrix
    0     1     2     3     4     5     6     7
    1 64023 12471  1916   188  1975  1684  4354
    2 27976 63465  9984  1019 11723  8441  1545
    3    88   137 12011    71   219   907     4
    4     0     0     0     0     0     0     0
    5    67   210   157    21  3862    89     2
    6    92   191   778    36   143  5419     5
    7   512    27    71     0     5    58 11069
accuracy 0.79604
loss 1.46856

neota:
f1_score 0.74628
auroc 0.61332
auprc 0.25167
recall 0.70352
precision 0.80622
confusion_matrix
    0     1     2     3     4     5     6     7
    1 13796  1718   720   190   518   567   905
    2  2308  4884   862   125   376   244   120
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7    37     1     9     1     0    11  1993
accuracy 0.75602
loss 1.52486

poudre:
f1_score 0.78386
auroc 0.63828
auprc 0.24893
recall 0.77930
precision 0.81357
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2    67   747  1440    14    24   611   103
    3   347   384 16208   462   192  2411   153
    4     9     4    30   520     1    23     0
    5     0     0     0     0     0     0     0
    6   133    89   427   122    22  7691    59
    7     0     0     0     0     0     0     0
accuracy 0.70968
loss 1.02990

rawah:
f1_score 0.73658
auroc 0.69248
auprc 0.25168
recall 0.71718
precision 0.77668
confusion_matrix
     0      1      2      3      4      5      6      7
     1  76811  16155    701    135   1259    494   9100
     2  33096 100783   1358    221   7413   1147   1045
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     31    150      5      2   2730      6      1
     6      0      0      0      0      0      0      0
     7    342     13     34      2      3     27   4135
accuracy 0.81741
loss 1.24595

saving test results to cover/log/89/seed_27//130445/test_results.txt
Done federatedsame_yspace testing.
Loading federatedsame_yspace trainer for seed 28...(21/21)
Found option --force-consistent-target-space. Preloading training sets and extracting global target space ...
Label encodings before forcing consistent target space (these will NOT be used)
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
... Now reloading with obtained global target space: [1. 2. 3. 4. 5. 6. 7.]:
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 4, 5, 6]
neota label encodings [1, 2, 7] => [0, 1, 6]
poudre label encodings [2, 3, 4, 6] => [1, 2, 3, 5]
rawah label encodings [1, 2, 5, 7] => [0, 1, 4, 6]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(7,)
name:cover_neota,input_shape:(11,):output_shape(7,)
name:cover_poudre,input_shape:(11,):output_shape(7,)
name:cover_rawah,input_shape:(11,):output_shape(7,)
Loaded SummaryWriter at cover/log/89/seed_28//130823/20230409_federatedsame_yspace_cover_split_by_wilderness_model_28/
Starting federatedsame_yspace training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.974 valid 0.824 comanche:auroc: train 0.846 valid 0.812 comanche:auprc: train 0.596 valid 0.518 comanche:recall: train 0.974 valid 0.824 comanche:precision: train 0.975 valid 0.824 comanche:accuracy: train 0.972 valid 0.807 comanche:loss: train 0.075 valid 0.972 neota:f1_score: train 0.983 valid 0.851 neota:auroc: train 0.656 valid 0.616 neota:auprc: train 0.336 valid 0.279 neota:recall: train 0.982 valid 0.850 neota:precision: train 0.984 valid 0.854 neota:accuracy: train 0.971 valid 0.770 neota:loss: train 0.148 valid 1.321 poudre:f1_score: train 0.995 valid 0.913 poudre:auroc: train 0.720 valid 0.659 poudre:auprc: train 0.460 valid 0.337 poudre:recall: train 0.995 valid 0.913 poudre:precision: train 0.995 valid 0.915 poudre:accuracy: train 0.996 valid 0.721 poudre:loss: train 0.022 valid 0.559 rawah:f1_score: train 0.996 valid 0.825 rawah:auroc: train 0.759 valid 0.723 rawah:auprc: train 0.524 valid 0.448 rawah:recall: train 0.996 valid 0.825 rawah:precision: train 0.996 valid 0.827 rawah:accuracy: train 0.996 valid 0.849 rawah:loss: train 0.022 valid 1.144 

Done federatedsame_yspace training. Starting testing....
Test results:

comanche:
f1_score 0.65374
auroc 0.79337
auprc 0.43549
recall 0.63557
precision 0.72939
confusion_matrix
    0     1     2     3     4     5     6     7
    1 58491 13719  2541   395  1992  4494  4979
    2 22739 65917 10744  1549 10322 11387  1495
    3    76   184 11979   125   231   835     7
    4     0     0     0     0     0     0     0
    5    99   204   187     8  3841    64     5
    6    43   212   731    90   138  5448     2
    7   319    35    23     1     5    39 11320
accuracy 0.79179
loss 1.88852

neota:
f1_score 0.72150
auroc 0.61246
auprc 0.29321
recall 0.69593
precision 0.75780
confusion_matrix
    0     1     2     3     4     5     6     7
    1 12958  3063   234   162   866   273   858
    2  2743  5450   193    89   142   143   159
    3     0     0     0     0     0     0     0
    4     0     0     0     0     0     0     0
    5     0     0     0     0     0     0     0
    6     0     0     0     0     0     0     0
    7     9     0     0     0     0     1  2042
accuracy 0.76996
loss 1.73562

poudre:
f1_score 0.78205
auroc 0.64315
auprc 0.26210
recall 0.75567
precision 0.82462
confusion_matrix
    0     1     2     3     4     5     6     7
    1     0     0     0     0     0     0     0
    2   271   977  1102   147    16   394    99
    3   818   715 15772   916   190  1628   118
    4     3     0     8   566     0    10     0
    5     0     0     0     0     0     0     0
    6   416   219   516   217    24  7088    63
    7     0     0     0     0     0     0     0
accuracy 0.72535
loss 1.28422

rawah:
f1_score 0.74927
auroc 0.69999
auprc 0.33152
recall 0.73241
precision 0.78351
confusion_matrix
     0      1      2      3      4      5      6      7
     1  76903  17572    177     26   1188    451   8338
     2  29542 104409    547    150   9122    457    836
     3      0      0      0      0      0      0      0
     4      0      0      0      0      0      0      0
     5     41    106      1      3   2773      1      0
     6      0      0      0      0      0      0      0
     7    234     27      0      0      0      5   4290
accuracy 0.83606
loss 1.52895

saving test results to cover/log/89/seed_28//130823/test_results.txt
Done federatedsame_yspace testing.
Running for seeds [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]...
Loading orphaned trainer for seed 8...(1/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_8//131200/20230409_orphaned_cover_split_by_wilderness_model_8/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.992 valid 0.842 comanche:auroc: train 0.980 valid 0.933 comanche:auprc: train 0.843 valid 0.724 comanche:recall: train 0.992 valid 0.845 comanche:precision: train 0.992 valid 0.841 comanche:accuracy: train 0.991 valid 0.829 comanche:loss: train 0.026 valid 1.041 neota:f1_score: train 1.000 valid 0.873 neota:auroc: train 0.998 valid 0.954 neota:auprc: train 0.959 valid 0.872 neota:recall: train 1.000 valid 0.876 neota:precision: train 1.000 valid 0.871 neota:accuracy: train 1.000 valid 0.792 neota:loss: train 0.002 valid 1.239 poudre:f1_score: train 0.977 valid 0.908 poudre:auroc: train 0.967 valid 0.924 poudre:auprc: train 0.689 valid 0.647 poudre:recall: train 0.977 valid 0.909 poudre:precision: train 0.977 valid 0.908 poudre:accuracy: train 0.943 valid 0.757 poudre:loss: train 0.063 valid 0.494 rawah:f1_score: train 0.996 valid 0.834 rawah:auroc: train 0.960 valid 0.916 rawah:auprc: train 0.882 valid 0.820 rawah:recall: train 0.996 valid 0.836 rawah:precision: train 0.996 valid 0.834 rawah:accuracy: train 0.996 valid 0.858 rawah:loss: train 0.011 valid 1.394 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.72120
auroc 0.91863
auprc 0.65962
recall 0.70944
precision 0.76927
confusion_matrix
    0     1     2     3     4     5     6
    1 66063 12808   312  2260   440  4728
    2 23558 76055  4396 12275  6773  1096
    3    24   135 11815   318  1134    11
    4    44   162    73  4089    39     1
    5    22   123   501   217  5797     4
    6   282    31     0     6     0 11423
accuracy 0.83750
loss 2.24273

neota:
f1_score 0.76700
auroc 0.89592
auprc 0.82641
recall 0.77203
precision 0.77858
confusion_matrix
    0     1     2     3
    1 15638  1604  1172
    2  3650  5009   260
    3     6     7  2039
accuracy 0.80151
loss 2.24648

poudre:
f1_score 0.78493
auroc 0.88772
auprc 0.69357
recall 0.79197
precision 0.82871
confusion_matrix
    0     1     2     3     4
    1   564  1722   152   568
    2    67 17175  1176  1739
    3     0    17   563     7
    4    11   875   384  7273
accuracy 0.71254
loss 1.73346

rawah:
f1_score 0.77738
auroc 0.90232
auprc 0.69075
recall 0.76552
precision 0.80218
confusion_matrix
     0      1      2      3      4
     1  80458  17448   1369   5380
     2  26012 109424   9000    627
     3      8    116   2801      0
     4    303     35      9   4209
accuracy 0.85114
loss 2.11047

saving test results to cover/log/89/seed_8//131200/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 9...(2/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_9//131541/20230409_orphaned_cover_split_by_wilderness_model_9/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.981 valid 0.816 comanche:auroc: train 0.949 valid 0.912 comanche:auprc: train 0.734 valid 0.658 comanche:recall: train 0.981 valid 0.822 comanche:precision: train 0.981 valid 0.816 comanche:accuracy: train 0.980 valid 0.803 comanche:loss: train 0.057 valid 1.391 neota:f1_score: train 0.997 valid 0.881 neota:auroc: train 0.993 valid 0.960 neota:auprc: train 0.932 valid 0.859 neota:recall: train 0.997 valid 0.885 neota:precision: train 0.997 valid 0.879 neota:accuracy: train 0.994 valid 0.800 neota:loss: train 0.005 valid 0.905 poudre:f1_score: train 0.952 valid 0.888 poudre:auroc: train 0.962 valid 0.930 poudre:auprc: train 0.792 valid 0.703 poudre:recall: train 0.952 valid 0.889 poudre:precision: train 0.952 valid 0.891 poudre:accuracy: train 0.836 valid 0.704 poudre:loss: train 0.139 valid 0.407 rawah:f1_score: train 0.990 valid 0.832 rawah:auroc: train 0.993 valid 0.940 rawah:auprc: train 0.978 valid 0.859 rawah:recall: train 0.990 valid 0.833 rawah:precision: train 0.990 valid 0.831 rawah:accuracy: train 0.990 valid 0.853 rawah:loss: train 0.041 valid 1.338 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.69659
auroc 0.89108
auprc 0.54613
recall 0.68701
precision 0.75241
confusion_matrix
    0     1     2     3     4     5     6
    1 66589 12233   197  1998   505  5089
    2 28578 70282  4086 12439  7346  1422
    3    23   321 11412   578  1100     3
    4    68   148    76  4055    60     1
    5    19    73   467   145  5960     0
    6   296    35     3     3     0 11405
accuracy 0.82830
loss 2.55337

neota:
f1_score 0.79355
auroc 0.89566
auprc 0.74250
recall 0.79547
precision 0.80122
confusion_matrix
    0     1     2     3
    1 15689  1855   870
    2  2851  5641   427
    3     5     2  2045
accuracy 0.82702
loss 1.70666

poudre:
f1_score 0.77643
auroc 0.85349
auprc 0.69251
recall 0.78023
precision 0.82882
confusion_matrix
    0     1     2     3     4
    1   584  1235   145  1042
    2    50 16805  1292  2010
    3     0    23   555     9
    4     3   898   390  7252
accuracy 0.70559
loss 1.28251

rawah:
f1_score 0.76455
auroc 0.90314
auprc 0.66862
recall 0.75420
precision 0.78633
confusion_matrix
     0      1      2      3      4
     1  76917  20135    923   6680
     2  26650 110159   7387    867
     3     32    168   2725      0
     4    325     52      0   4179
accuracy 0.83581
loss 1.89842

saving test results to cover/log/89/seed_9//131541/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 10...(3/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_10//131917/20230409_orphaned_cover_split_by_wilderness_model_10/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.613 valid 0.557 comanche:auroc: train 0.528 valid 0.532 comanche:auprc: train 0.247 valid 0.248 comanche:recall: train 0.616 valid 0.573 comanche:precision: train 0.611 valid 0.629 comanche:accuracy: train 0.591 valid 0.562 comanche:loss: train 4.489 valid 6.397 neota:f1_score: train 0.952 valid 0.856 neota:auroc: train 0.871 valid 0.877 neota:auprc: train 0.712 valid 0.689 neota:recall: train 0.953 valid 0.850 neota:precision: train 0.952 valid 0.876 neota:accuracy: train 0.916 valid 0.820 neota:loss: train 0.134 valid 0.460 poudre:f1_score: train 0.979 valid 0.884 poudre:auroc: train 0.983 valid 0.957 poudre:auprc: train 0.839 valid 0.740 poudre:recall: train 0.979 valid 0.883 poudre:precision: train 0.979 valid 0.887 poudre:accuracy: train 0.877 valid 0.701 poudre:loss: train 0.060 valid 0.424 rawah:f1_score: train 0.986 valid 0.824 rawah:auroc: train 0.965 valid 0.909 rawah:auprc: train 0.890 valid 0.788 rawah:recall: train 0.986 valid 0.824 rawah:precision: train 0.986 valid 0.825 rawah:accuracy: train 0.987 valid 0.844 rawah:loss: train 0.041 valid 1.591 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.49865
auroc 0.57822
auprc 0.23271
recall 0.50700
precision 0.60305
confusion_matrix
    0     1     2     3     4     5     6
    1 18683 45526   686  2082  3219 16415
    2  7761 81275  8477 10855 11566  4219
    3    19   826  9374   425  2793     0
    4    87  2114   665  1144   398     0
    5     5   824  2006   120  3709     0
    6   253   334     0    13    91 11051
accuracy 0.55420
loss 1713.26636

neota:
f1_score 0.75259
auroc 0.71283
auprc 0.66762
recall 0.74732
precision 0.78269
confusion_matrix
    0     1     2     3
    1 12899  5025   490
    2  1695  7055   169
    3    14    32  2006
accuracy 0.82303
loss 0.83977

poudre:
f1_score 0.79645
auroc 0.87384
auprc 0.64902
recall 0.80113
precision 0.83251
confusion_matrix
    0     1     2     3     4
    1   813  1420    11   762
    2    75 16814  1060  2208
    3     0    14   559    14
    4    63   601   194  7685
accuracy 0.73912
loss 2.08252

rawah:
f1_score 0.77489
auroc 0.88497
auprc 0.62219
recall 0.76562
precision 0.79509
confusion_matrix
     0      1      2      3      4
     1  78544  18682    947   6482
     2  26392 111504   5899   1268
     3     21    205   2693      6
     4    330     50      0   4176
accuracy 0.83911
loss 1.69008

saving test results to cover/log/89/seed_10//131917/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 11...(4/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_11//132256/20230409_orphaned_cover_split_by_wilderness_model_11/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.975 valid 0.826 comanche:auroc: train 0.984 valid 0.938 comanche:auprc: train 0.895 valid 0.739 comanche:recall: train 0.975 valid 0.830 comanche:precision: train 0.975 valid 0.825 comanche:accuracy: train 0.973 valid 0.812 comanche:loss: train 0.070 valid 1.129 neota:f1_score: train 0.971 valid 0.876 neota:auroc: train 0.985 valid 0.952 neota:auprc: train 0.927 valid 0.831 neota:recall: train 0.971 valid 0.876 neota:precision: train 0.971 valid 0.879 neota:accuracy: train 0.950 valid 0.826 neota:loss: train 0.093 valid 0.619 poudre:f1_score: train 0.988 valid 0.903 poudre:auroc: train 0.981 valid 0.919 poudre:auprc: train 0.882 valid 0.693 poudre:recall: train 0.988 valid 0.905 poudre:precision: train 0.988 valid 0.905 poudre:accuracy: train 0.954 valid 0.712 poudre:loss: train 0.031 valid 0.627 rawah:f1_score: train 0.975 valid 0.827 rawah:auroc: train 0.957 valid 0.914 rawah:auprc: train 0.887 valid 0.805 rawah:recall: train 0.975 valid 0.828 rawah:precision: train 0.975 valid 0.828 rawah:accuracy: train 0.978 valid 0.849 rawah:loss: train 0.077 valid 1.239 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.70906
auroc 0.91430
auprc 0.59876
recall 0.69690
precision 0.74942
confusion_matrix
    0     1     2     3     4     5     6
    1 59621 18027   185  2400   614  5764
    2 20049 79539  4445 10743  8242  1135
    3    48   146 11882   225  1116    20
    4    55   273   114  3882    83     1
    5    35   145   561   156  5764     3
    6   229    49     1     4     2 11457
accuracy 0.82244
loss 1.93832

neota:
f1_score 0.77336
auroc 0.87996
auprc 0.83062
recall 0.77471
precision 0.78290
confusion_matrix
    0     1     2     3
    1 15220  1976  1218
    2  3135  5505   279
    3    12     0  2040
accuracy 0.81264
loss 1.12568

poudre:
f1_score 0.77727
auroc 0.85343
auprc 0.65682
recall 0.78500
precision 0.83188
confusion_matrix
    0     1     2     3     4
    1   527  1651    69   759
    2     6 16891  1307  1953
    3     0    11   566    10
    4    10   833   334  7366
accuracy 0.70993
loss 2.03225

rawah:
f1_score 0.77040
auroc 0.85416
auprc 0.58142
recall 0.76099
precision 0.79022
confusion_matrix
     0      1      2      3      4
     1  75826  21104    944   6781
     2  24575 112918   6761    809
     3     19    177   2729      0
     4    270     32      0   4254
accuracy 0.84241
loss 1.36823

saving test results to cover/log/89/seed_11//132256/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 12...(5/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_12//132636/20230409_orphaned_cover_split_by_wilderness_model_12/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.988 valid 0.833 comanche:auroc: train 0.981 valid 0.927 comanche:auprc: train 0.838 valid 0.684 comanche:recall: train 0.988 valid 0.837 comanche:precision: train 0.988 valid 0.831 comanche:accuracy: train 0.987 valid 0.819 comanche:loss: train 0.036 valid 1.220 neota:f1_score: train 0.869 valid 0.810 neota:auroc: train 0.552 valid 0.545 neota:auprc: train 0.437 valid 0.424 neota:recall: train 0.882 valid 0.832 neota:precision: train 0.879 valid 0.814 neota:accuracy: train 0.765 valid 0.675 neota:loss: train 0.283 valid 2.278 poudre:f1_score: train 0.994 valid 0.918 poudre:auroc: train 0.976 valid 0.867 poudre:auprc: train 0.910 valid 0.678 poudre:recall: train 0.994 valid 0.919 poudre:precision: train 0.994 valid 0.917 poudre:accuracy: train 0.995 valid 0.766 poudre:loss: train 0.022 valid 0.673 rawah:f1_score: train 0.989 valid 0.814 rawah:auroc: train 0.970 valid 0.924 rawah:auprc: train 0.916 valid 0.838 rawah:recall: train 0.989 valid 0.816 rawah:precision: train 0.989 valid 0.817 rawah:accuracy: train 0.989 valid 0.837 rawah:loss: train 0.031 valid 1.269 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.70433
auroc 0.91817
auprc 0.60357
recall 0.69167
precision 0.75773
confusion_matrix
    0     1     2     3     4     5     6
    1 63665 12513   159  2651   329  7294
    2 25376 74109  3924 13162  5700  1882
    3    21   369 11531   392  1116     8
    4    34   160    69  4101    44     0
    5    12   147   519   148  5837     1
    6   117    11     0     4     0 11610
accuracy 0.83086
loss 2.22266

neota:
f1_score 0.59956
auroc 0.63274
auprc 0.45143
recall 0.65387
precision 0.63442
confusion_matrix
    0     1     2     3
    1 15840  1120  1454
    2  7186  1388   345
    3    65     1  1986
accuracy 0.66122
loss 11.06286

poudre:
f1_score 0.78396
auroc 0.87224
auprc 0.65389
recall 0.79429
precision 0.82647
confusion_matrix
    0     1     2     3     4
    1   484  1691    51   780
    2    91 17003  1169  1894
    3     0    10   574     3
    4    13   615   326  7589
accuracy 0.71768
loss 2.39478

rawah:
f1_score 0.76949
auroc 0.87932
auprc 0.64615
recall 0.76200
precision 0.78693
confusion_matrix
     0      1      2      3      4
     1  72238  25628   1417   5372
     2  20715 116850   6923    575
     3     32    106   2787      0
     4    389     54      2   4111
accuracy 0.83773
loss 1.88167

saving test results to cover/log/89/seed_12//132636/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 13...(6/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_13//133016/20230409_orphaned_cover_split_by_wilderness_model_13/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.987 valid 0.835 comanche:auroc: train 0.970 valid 0.920 comanche:auprc: train 0.807 valid 0.686 comanche:recall: train 0.987 valid 0.837 comanche:precision: train 0.987 valid 0.834 comanche:accuracy: train 0.987 valid 0.821 comanche:loss: train 0.040 valid 1.125 neota:f1_score: train 0.997 valid 0.872 neota:auroc: train 0.991 valid 0.923 neota:auprc: train 0.988 valid 0.813 neota:recall: train 0.997 valid 0.876 neota:precision: train 0.997 valid 0.869 neota:accuracy: train 0.993 valid 0.776 neota:loss: train 0.005 valid 1.286 poudre:f1_score: train 0.990 valid 0.890 poudre:auroc: train 0.989 valid 0.933 poudre:auprc: train 0.949 valid 0.720 poudre:recall: train 0.990 valid 0.892 poudre:precision: train 0.990 valid 0.890 poudre:accuracy: train 0.974 valid 0.703 poudre:loss: train 0.042 valid 0.685 rawah:f1_score: train 0.993 valid 0.830 rawah:auroc: train 0.983 valid 0.928 rawah:auprc: train 0.947 valid 0.850 rawah:recall: train 0.993 valid 0.831 rawah:precision: train 0.993 valid 0.830 rawah:accuracy: train 0.994 valid 0.849 rawah:loss: train 0.027 valid 1.111 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.72325
auroc 0.92194
auprc 0.63852
recall 0.71162
precision 0.76778
confusion_matrix
    0     1     2     3     4     5     6
    1 64751 13623   287  2395   496  5059
    2 21917 77915  4657 11225  7203  1236
    3    10   209 12023   253   934     8
    4    32   225    97  4013    41     0
    5     7   154   689    93  5721     0
    6   317    61     0     5     0 11359
accuracy 0.83437
loss 2.03337

neota:
f1_score 0.77316
auroc 0.90877
auprc 0.86640
recall 0.78030
precision 0.78584
confusion_matrix
    0     1     2     3
    1 16017  1384  1013
    2  3779  4877   263
    3     9     8  2035
accuracy 0.80278
loss 2.18972

poudre:
f1_score 0.78363
auroc 0.85386
auprc 0.63737
recall 0.78534
precision 0.82659
confusion_matrix
    0     1     2     3     4
    1   721  1515   136   634
    2    76 16707  1288  2086
    3     0    15   562    10
    4    36   771   365  7371
accuracy 0.72223
loss 3.65321

rawah:
f1_score 0.77535
auroc 0.91004
auprc 0.67153
recall 0.76480
precision 0.79653
confusion_matrix
     0      1      2      3      4
     1  77414  20147   1475   5619
     2  24183 112286   8116    478
     3     16     95   2814      0
     4    287     72      5   4192
accuracy 0.84898
loss 1.78422

saving test results to cover/log/89/seed_13//133016/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 14...(7/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_14//133400/20230409_orphaned_cover_split_by_wilderness_model_14/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.968 valid 0.833 comanche:auroc: train 0.964 valid 0.920 comanche:auprc: train 0.797 valid 0.709 comanche:recall: train 0.968 valid 0.836 comanche:precision: train 0.968 valid 0.833 comanche:accuracy: train 0.965 valid 0.817 comanche:loss: train 0.118 valid 0.899 neota:f1_score: train 0.871 valid 0.834 neota:auroc: train 0.814 valid 0.839 neota:auprc: train 0.582 valid 0.611 neota:recall: train 0.876 valid 0.850 neota:precision: train 0.869 valid 0.842 neota:accuracy: train 0.774 valid 0.710 neota:loss: train 0.335 valid 0.467 poudre:f1_score: train 0.985 valid 0.904 poudre:auroc: train 0.968 valid 0.882 poudre:auprc: train 0.910 valid 0.683 poudre:recall: train 0.985 valid 0.907 poudre:precision: train 0.985 valid 0.905 poudre:accuracy: train 0.970 valid 0.712 poudre:loss: train 0.053 valid 0.537 rawah:f1_score: train 0.983 valid 0.819 rawah:auroc: train 0.950 valid 0.883 rawah:auprc: train 0.861 valid 0.744 rawah:recall: train 0.983 valid 0.820 rawah:precision: train 0.983 valid 0.819 rawah:accuracy: train 0.984 valid 0.843 rawah:loss: train 0.051 valid 1.137 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.71432
auroc 0.90959
auprc 0.61953
recall 0.70079
precision 0.75542
confusion_matrix
    0     1     2     3     4     5     6
    1 56282 20176   372  2375   657  6749
    2 15962 83981  4008 12075  6738  1389
    3    23   327 11284   414  1389     0
    4    37   208    86  4016    61     0
    5    21   146   418   121  5958     0
    6   110    41     0     6     0 11585
accuracy 0.82630
loss 1.78315

neota:
f1_score 0.71829
auroc 0.76463
auprc 0.67639
recall 0.74269
precision 0.74241
confusion_matrix
    0     1     2     3
    1 16501  1166   747
    2  5516  3307    96
    3    32     4  2016
accuracy 0.74978
loss 0.75463

poudre:
f1_score 0.77034
auroc 0.85791
auprc 0.66897
recall 0.78054
precision 0.82531
confusion_matrix
    0     1     2     3     4
    1   410  1732    96   768
    2    37 16780  1316  2024
    3     0     6   575     6
    4     4   736   362  7441
accuracy 0.70486
loss 2.05512

rawah:
f1_score 0.76099
auroc 0.90531
auprc 0.68748
recall 0.75010
precision 0.78548
confusion_matrix
     0      1      2      3      4
     1  78173  18178   1296   7008
     2  29404 107827   6740   1092
     3     33    205   2685      2
     4    274     41      0   4241
accuracy 0.83477
loss 1.49335

saving test results to cover/log/89/seed_14//133400/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 15...(8/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_15//133742/20230409_orphaned_cover_split_by_wilderness_model_15/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.990 valid 0.829 comanche:auroc: train 0.980 valid 0.939 comanche:auprc: train 0.880 valid 0.765 comanche:recall: train 0.990 valid 0.831 comanche:precision: train 0.990 valid 0.827 comanche:accuracy: train 0.989 valid 0.814 comanche:loss: train 0.035 valid 1.127 neota:f1_score: train 0.617 valid 0.502 neota:auroc: train 0.577 valid 0.590 neota:auprc: train 0.464 valid 0.448 neota:recall: train 0.637 valid 0.504 neota:precision: train 0.609 valid 0.588 neota:accuracy: train 0.518 valid 0.520 neota:loss: train 6.870 valid 2.340 poudre:f1_score: train 0.988 valid 0.896 poudre:auroc: train 0.985 valid 0.953 poudre:auprc: train 0.788 valid 0.709 poudre:recall: train 0.988 valid 0.899 poudre:precision: train 0.988 valid 0.902 poudre:accuracy: train 0.955 valid 0.709 poudre:loss: train 0.044 valid 0.507 rawah:f1_score: train 0.988 valid 0.830 rawah:auroc: train 0.975 valid 0.922 rawah:auprc: train 0.936 valid 0.835 rawah:recall: train 0.988 valid 0.832 rawah:precision: train 0.988 valid 0.833 rawah:accuracy: train 0.990 valid 0.853 rawah:loss: train 0.033 valid 1.600 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.73158
auroc 0.91789
auprc 0.61016
recall 0.72023
precision 0.76808
confusion_matrix
    0     1     2     3     4     5     6
    1 63892 15814   238  1911   362  4394
    2 20356 81172  4043 11413  6224   945
    3    12   347 11783   316   976     3
    4    45   240   133  3941    49     0
    5    14   182   567   182  5710     9
    6   286    39     2     5     1 11409
accuracy 0.83182
loss 1.99797

neota:
f1_score 0.28588
auroc 0.65052
auprc 0.54492
recall 0.37172
precision 0.56768
confusion_matrix
    0     1     2     3
    1  1749 14792  1873
    2   490  7602   827
    3   252   228  1572
accuracy 0.57113
loss 29.52379

poudre:
f1_score 0.76921
auroc 0.86908
auprc 0.67424
recall 0.76729
precision 0.82760
confusion_matrix
    0     1     2     3     4
    1   719  1139    95  1053
    2    78 15752  1481  2846
    3     0     9   569     9
    4    45   404   356  7738
accuracy 0.72394
loss 1.63014

rawah:
f1_score 0.77515
auroc 0.91062
auprc 0.64587
recall 0.76619
precision 0.79389
confusion_matrix
     0      1      2      3      4
     1  75045  22652   1346   5612
     2  22097 115017   7323    626
     3     29    126   2770      0
     4    268     55      1   4232
accuracy 0.84646
loss 2.05085

saving test results to cover/log/89/seed_15//133742/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 16...(9/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_16//134121/20230409_orphaned_cover_split_by_wilderness_model_16/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.988 valid 0.833 comanche:auroc: train 0.993 valid 0.942 comanche:auprc: train 0.959 valid 0.814 comanche:recall: train 0.988 valid 0.835 comanche:precision: train 0.988 valid 0.832 comanche:accuracy: train 0.987 valid 0.819 comanche:loss: train 0.037 valid 1.140 neota:f1_score: train 0.919 valid 0.842 neota:auroc: train 0.509 valid 0.489 neota:auprc: train 0.445 valid 0.438 neota:recall: train 0.924 valid 0.850 neota:precision: train 0.923 valid 0.842 neota:accuracy: train 0.837 valid 0.741 neota:loss: train 0.164 valid 0.486 poudre:f1_score: train 0.989 valid 0.894 poudre:auroc: train 0.942 valid 0.835 poudre:auprc: train 0.839 valid 0.623 poudre:recall: train 0.989 valid 0.895 poudre:precision: train 0.989 valid 0.899 poudre:accuracy: train 0.990 valid 0.708 poudre:loss: train 0.031 valid 0.603 rawah:f1_score: train 0.993 valid 0.821 rawah:auroc: train 0.953 valid 0.881 rawah:auprc: train 0.861 valid 0.765 rawah:recall: train 0.993 valid 0.823 rawah:precision: train 0.993 valid 0.821 rawah:accuracy: train 0.994 valid 0.846 rawah:loss: train 0.030 valid 2.095 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.73123
auroc 0.92643
auprc 0.68163
recall 0.71797
precision 0.76907
confusion_matrix
    0     1     2     3     4     5     6
    1 61578 17661   140  2103   799  4330
    2 17505 82990  3413 11497  7327  1421
    3    46   363 11407   467  1133    21
    4    31   233    84  4013    44     3
    5    17   133   455   106  5940    13
    6   288    27     1     4     0 11422
accuracy 0.83381
loss 2.17973

neota:
f1_score 0.67964
auroc 0.65036
auprc 0.60357
recall 0.69195
precision 0.69415
confusion_matrix
    0     1     2     3
    1 14686  2080  1648
    2  5079  3615   225
    3    20     0  2032
accuracy 0.73104
loss 1.02395

poudre:
f1_score 0.76899
auroc 0.84374
auprc 0.66662
recall 0.77658
precision 0.82841
confusion_matrix
    0     1     2     3     4
    1   571  1463   110   862
    2    27 16121   989  3020
    3     0    12   569     6
    4     6   422   298  7817
accuracy 0.71852
loss 3.41663

rawah:
f1_score 0.77225
auroc 0.89549
auprc 0.66849
recall 0.75981
precision 0.79737
confusion_matrix
     0      1      2      3      4
     1  76903  19122   1505   7125
     2  24479 111444   7966   1174
     3     19     93   2813      0
     4    273     21      0   4262
accuracy 0.85006
loss 2.26479

saving test results to cover/log/89/seed_16//134121/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 17...(10/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_17//134458/20230409_orphaned_cover_split_by_wilderness_model_17/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.982 valid 0.825 comanche:auroc: train 0.981 valid 0.934 comanche:auprc: train 0.864 valid 0.736 comanche:recall: train 0.982 valid 0.828 comanche:precision: train 0.982 valid 0.825 comanche:accuracy: train 0.980 valid 0.810 comanche:loss: train 0.050 valid 1.182 neota:f1_score: train 0.895 valid 0.783 neota:auroc: train 0.958 valid 0.900 neota:auprc: train 0.917 valid 0.781 neota:recall: train 0.895 valid 0.788 neota:precision: train 0.897 valid 0.782 neota:accuracy: train 0.869 valid 0.723 neota:loss: train 0.848 valid 3.132 poudre:f1_score: train 0.989 valid 0.888 poudre:auroc: train 0.999 valid 0.936 poudre:auprc: train 0.996 valid 0.747 poudre:recall: train 0.989 valid 0.889 poudre:precision: train 0.989 valid 0.888 poudre:accuracy: train 0.991 valid 0.699 poudre:loss: train 0.034 valid 0.633 rawah:f1_score: train 0.985 valid 0.830 rawah:auroc: train 0.971 valid 0.918 rawah:auprc: train 0.908 valid 0.811 rawah:recall: train 0.985 valid 0.831 rawah:precision: train 0.985 valid 0.830 rawah:accuracy: train 0.988 valid 0.851 rawah:loss: train 0.038 valid 1.298 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.71397
auroc 0.91126
auprc 0.63609
recall 0.70131
precision 0.75300
confusion_matrix
    0     1     2     3     4     5     6
    1 58322 19625   420  2355   708  5181
    2 17805 81839  4299 11251  7956  1003
    3    15   156 11747   305  1213     1
    4    22   233   123  3955    74     1
    5    16    95   504    84  5964     1
    6   291    31     4     7     1 11408
accuracy 0.82842
loss 2.15220

neota:
f1_score 0.58855
auroc 0.77050
auprc 0.64973
recall 0.58220
precision 0.62708
confusion_matrix
    0     1     2     3
    1 11861  2975  3578
    2  4662  3361   896
    3   162     4  1886
accuracy 0.64669
loss 7.65464

poudre:
f1_score 0.78932
auroc 0.87251
auprc 0.67534
recall 0.79370
precision 0.82941
confusion_matrix
    0     1     2     3     4
    1   708  1576    78   644
    2    80 16775  1145  2157
    3     0    10   571     6
    4    48   585   333  7577
accuracy 0.73185
loss 2.03866

rawah:
f1_score 0.77814
auroc 0.88864
auprc 0.65048
recall 0.76863
precision 0.79777
confusion_matrix
     0      1      2      3      4
     1  77598  19984   1176   5897
     2  24128 113152   7184    599
     3     17    123   2785      0
     4    381     20      0   4155
accuracy 0.84640
loss 1.92282

saving test results to cover/log/89/seed_17//134458/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 18...(11/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_18//134839/20230409_orphaned_cover_split_by_wilderness_model_18/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.845 valid 0.782 comanche:auroc: train 0.841 valid 0.831 comanche:auprc: train 0.574 valid 0.543 comanche:recall: train 0.848 valid 0.785 comanche:precision: train 0.844 valid 0.782 comanche:accuracy: train 0.836 valid 0.767 comanche:loss: train 0.404 valid 0.659 neota:f1_score: train 0.997 valid 0.795 neota:auroc: train 0.910 valid 0.788 neota:auprc: train 0.676 valid 0.544 neota:recall: train 0.997 valid 0.805 neota:precision: train 0.997 valid 0.787 neota:accuracy: train 0.993 valid 0.664 neota:loss: train 0.016 valid 10.583 poudre:f1_score: train 0.985 valid 0.893 poudre:auroc: train 0.938 valid 0.878 poudre:auprc: train 0.737 valid 0.581 poudre:recall: train 0.985 valid 0.895 poudre:precision: train 0.986 valid 0.895 poudre:accuracy: train 0.952 valid 0.704 poudre:loss: train 0.064 valid 0.644 rawah:f1_score: train 0.993 valid 0.817 rawah:auroc: train 0.978 valid 0.919 rawah:auprc: train 0.929 valid 0.812 rawah:recall: train 0.993 valid 0.819 rawah:precision: train 0.993 valid 0.818 rawah:accuracy: train 0.992 valid 0.843 rawah:loss: train 0.022 valid 1.603 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.66552
auroc 0.82485
auprc 0.46697
recall 0.65212
precision 0.71093
confusion_matrix
    0     1     2     3     4     5     6
    1 55340 21254   175  2525   371  6946
    2 21740 73435  5907 14250  7400  1421
    3     0   217 12001   269   942     8
    4    18   344   213  3710   123     0
    5     2   234   843   121  5462     2
    6   523    74     4     6     0 11135
accuracy 0.78886
loss 11912.81055

neota:
f1_score 0.69399
auroc 0.75985
auprc 0.56966
recall 0.69910
precision 0.70638
confusion_matrix
    0     1     2     3
    1 14284  2509  1621
    2  4316  4229   374
    3    20     2  2030
accuracy 0.74638
loss 11.46077

poudre:
f1_score 0.77027
auroc 0.84890
auprc 0.61997
recall 0.77295
precision 0.81777
confusion_matrix
    0     1     2     3     4
    1   586  1666    66   688
    2   101 16265  1513  2278
    3     0     8   572     7
    4    34   577   394  7538
accuracy 0.71467
loss 2.13772

rawah:
f1_score 0.77016
auroc 0.90334
auprc 0.67673
recall 0.75864
precision 0.79393
confusion_matrix
     0      1      2      3      4
     1  74435  20233   1223   8764
     2  23895 113580   6537   1051
     3     12    135   2778      0
     4    195     32      0   4329
accuracy 0.84853
loss 2.22971

saving test results to cover/log/89/seed_18//134839/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 19...(12/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_19//135216/20230409_orphaned_cover_split_by_wilderness_model_19/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.678 valid 0.680 comanche:auroc: train 0.691 valid 0.688 comanche:auprc: train 0.370 valid 0.363 comanche:recall: train 0.680 valid 0.688 comanche:precision: train 0.678 valid 0.681 comanche:accuracy: train 0.661 valid 0.669 comanche:loss: train 0.974 valid 0.992 neota:f1_score: train 0.968 valid 0.848 neota:auroc: train 0.988 valid 0.944 neota:auprc: train 0.960 valid 0.785 neota:recall: train 0.968 valid 0.850 neota:precision: train 0.968 valid 0.850 neota:accuracy: train 0.942 valid 0.770 neota:loss: train 0.128 valid 0.900 poudre:f1_score: train 0.983 valid 0.904 poudre:auroc: train 0.981 valid 0.878 poudre:auprc: train 0.866 valid 0.711 poudre:recall: train 0.983 valid 0.906 poudre:precision: train 0.983 valid 0.904 poudre:accuracy: train 0.969 valid 0.714 poudre:loss: train 0.055 valid 0.632 rawah:f1_score: train 0.984 valid 0.834 rawah:auroc: train 0.974 valid 0.931 rawah:auprc: train 0.910 valid 0.838 rawah:recall: train 0.984 valid 0.835 rawah:precision: train 0.984 valid 0.835 rawah:accuracy: train 0.984 valid 0.855 rawah:loss: train 0.061 valid 1.333 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.49671
auroc 0.72170
auprc 0.34522
recall 0.47391
precision 0.60635
confusion_matrix
    0     1     2     3     4     5     6
    1 48716 19363   573  7075  2244  8640
    2 30359 39937  6512 31095 12295  3955
    3    11    43  9270  1127  2976    10
    4   138   455   328  2978   478    31
    5    43    88   838   366  5323     6
    6   761    64    25    26    26 10840
accuracy 0.66193
loss 1.92056

neota:
f1_score 0.74823
auroc 0.87399
auprc 0.81044
recall 0.74627
precision 0.75736
confusion_matrix
    0     1     2     3
    1 13997  3373  1044
    2  2802  5976   141
    3    28    68  1956
accuracy 0.79446
loss 1.67781

poudre:
f1_score 0.79171
auroc 0.87957
auprc 0.75061
recall 0.79126
precision 0.84443
confusion_matrix
    0     1     2     3     4
    1   693  1476   136   701
    2    42 16690  1636  1789
    3     1     3   574     9
    4    12   593   343  7595
accuracy 0.73136
loss 2.17873

rawah:
f1_score 0.77555
auroc 0.87910
auprc 0.61403
recall 0.76557
precision 0.79574
confusion_matrix
     0      1      2      3      4
     1  76703  20631   1299   6022
     2  23600 113330   7279    854
     3     22    117   2786      0
     4    405     65      0   4086
accuracy 0.84087
loss 1.76491

saving test results to cover/log/89/seed_19//135216/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 20...(13/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_20//135554/20230409_orphaned_cover_split_by_wilderness_model_20/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.988 valid 0.837 comanche:auroc: train 0.981 valid 0.935 comanche:auprc: train 0.854 valid 0.717 comanche:recall: train 0.988 valid 0.840 comanche:precision: train 0.988 valid 0.836 comanche:accuracy: train 0.987 valid 0.823 comanche:loss: train 0.037 valid 1.226 neota:f1_score: train 0.990 valid 0.873 neota:auroc: train 0.608 valid 0.613 neota:auprc: train 0.465 valid 0.462 neota:recall: train 0.989 valid 0.876 neota:precision: train 0.990 valid 0.872 neota:accuracy: train 0.986 valid 0.779 neota:loss: train 0.029 valid 0.533 poudre:f1_score: train 0.989 valid 0.901 poudre:auroc: train 0.908 valid 0.811 poudre:auprc: train 0.730 valid 0.545 poudre:recall: train 0.989 valid 0.903 poudre:precision: train 0.989 valid 0.901 poudre:accuracy: train 0.973 valid 0.711 poudre:loss: train 0.056 valid 0.579 rawah:f1_score: train 0.979 valid 0.813 rawah:auroc: train 0.975 valid 0.921 rawah:auprc: train 0.935 valid 0.828 rawah:recall: train 0.979 valid 0.816 rawah:precision: train 0.979 valid 0.813 rawah:accuracy: train 0.979 valid 0.840 rawah:loss: train 0.064 valid 1.023 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.72092
auroc 0.92361
auprc 0.62125
recall 0.70658
precision 0.76523
confusion_matrix
    0     1     2     3     4     5     6
    1 61474 16341   413  2542   685  5156
    2 18461 79862  4719 12808  7205  1098
    3     8   240 11747   351  1081    10
    4    18   185    80  4072    52     1
    5     6   123   480   138  5896    21
    6   212    33     1     5     6 11485
accuracy 0.83565
loss 2.29462

neota:
f1_score 0.73733
auroc 0.71062
auprc 0.60840
recall 0.74194
precision 0.73979
confusion_matrix
    0     1     2     3
    1 14946  2780   688
    2  3706  4826   387
    3    20     2  2030
accuracy 0.78068
loss 4.33127

poudre:
f1_score 0.79899
auroc 0.85114
auprc 0.69666
recall 0.79940
precision 0.83679
confusion_matrix
    0     1     2     3     4
    1   840  1573   138   455
    2    60 17310  1225  1562
    3     0     5   575     7
    4    44   983   426  7090
accuracy 0.73692
loss 2.39349

rawah:
f1_score 0.75947
auroc 0.86759
auprc 0.66306
recall 0.74729
precision 0.78473
confusion_matrix
     0      1      2      3      4
     1  75906  20102   1259   7388
     2  26655 109226   8029   1153
     3     30    103   2792      0
     4    245     32      1   4278
accuracy 0.84294
loss 1.38274

saving test results to cover/log/89/seed_20//135554/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 21...(14/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_21//135931/20230409_orphaned_cover_split_by_wilderness_model_21/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.970 valid 0.831 comanche:auroc: train 0.956 valid 0.917 comanche:auprc: train 0.776 valid 0.684 comanche:recall: train 0.970 valid 0.833 comanche:precision: train 0.970 valid 0.830 comanche:accuracy: train 0.967 valid 0.817 comanche:loss: train 0.082 valid 0.999 neota:f1_score: train 0.997 valid 0.881 neota:auroc: train 0.983 valid 0.930 neota:auprc: train 0.912 valid 0.798 neota:recall: train 0.997 valid 0.885 neota:precision: train 0.997 valid 0.879 neota:accuracy: train 0.993 valid 0.800 neota:loss: train 0.005 valid 1.118 poudre:f1_score: train 0.997 valid 0.917 poudre:auroc: train 0.998 valid 0.903 poudre:auprc: train 0.994 valid 0.787 poudre:recall: train 0.997 valid 0.919 poudre:precision: train 0.997 valid 0.918 poudre:accuracy: train 0.997 valid 0.766 poudre:loss: train 0.008 valid 0.711 rawah:f1_score: train 0.988 valid 0.820 rawah:auroc: train 0.913 valid 0.852 rawah:auprc: train 0.782 valid 0.707 rawah:recall: train 0.988 valid 0.822 rawah:precision: train 0.988 valid 0.821 rawah:accuracy: train 0.988 valid 0.848 rawah:loss: train 0.044 valid 1.518 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.71236
auroc 0.91730
auprc 0.61480
recall 0.70060
precision 0.75617
confusion_matrix
    0     1     2     3     4     5     6
    1 64799 15457   190  1889   595  3681
    2 23391 75564  4342 12551  7499   806
    3     9   263 11671   376  1118     0
    4    72   148   106  4002    77     3
    5    27   114   479   135  5907     2
    6   533    88     0     6     0 11115
accuracy 0.82771
loss 1.81896

neota:
f1_score 0.78894
auroc 0.85233
auprc 0.79979
recall 0.79061
precision 0.79944
confusion_matrix
    0     1     2     3
    1 15583  1648  1183
    2  3047  5598   274
    3     0     1  2051
accuracy 0.82447
loss 1.80174

poudre:
f1_score 0.80865
auroc 0.87734
auprc 0.73580
recall 0.81092
precision 0.84829
confusion_matrix
    0     1     2     3     4
    1   818  1708    67   413
    2    45 17356  1261  1495
    3     0     7   572     8
    4    20   732   350  7441
accuracy 0.74465
loss 2.50494

rawah:
f1_score 0.75703
auroc 0.88004
auprc 0.62565
recall 0.74330
precision 0.78763
confusion_matrix
     0      1      2      3      4
     1  77584  16199   1556   9316
     2  30679 106580   6830    974
     3      8    157   2760      0
     4    227     75      2   4252
accuracy 0.83823
loss 1.77391

saving test results to cover/log/89/seed_21//135931/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 22...(15/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_22//140314/20230409_orphaned_cover_split_by_wilderness_model_22/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.995 valid 0.839 comanche:auroc: train 0.965 valid 0.913 comanche:auprc: train 0.802 valid 0.700 comanche:recall: train 0.995 valid 0.842 comanche:precision: train 0.995 valid 0.838 comanche:accuracy: train 0.995 valid 0.824 comanche:loss: train 0.017 valid 1.556 neota:f1_score: train 0.955 valid 0.871 neota:auroc: train 0.984 valid 0.945 neota:auprc: train 0.955 valid 0.820 neota:recall: train 0.955 valid 0.876 neota:precision: train 0.955 valid 0.881 neota:accuracy: train 0.941 valid 0.784 neota:loss: train 0.192 valid 0.533 poudre:f1_score: train 0.986 valid 0.887 poudre:auroc: train 0.903 valid 0.839 poudre:auprc: train 0.740 valid 0.542 poudre:recall: train 0.986 valid 0.889 poudre:precision: train 0.986 valid 0.887 poudre:accuracy: train 0.971 valid 0.697 poudre:loss: train 0.050 valid 0.575 rawah:f1_score: train 0.975 valid 0.819 rawah:auroc: train 0.980 valid 0.917 rawah:auprc: train 0.931 valid 0.815 rawah:recall: train 0.975 valid 0.821 rawah:precision: train 0.975 valid 0.819 rawah:accuracy: train 0.976 valid 0.848 rawah:loss: train 0.080 valid 1.213 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.71370
auroc 0.90187
auprc 0.63063
recall 0.69828
precision 0.75821
confusion_matrix
    0     1     2     3     4     5     6
    1 62640 16986   178  2830   465  3512
    2 21237 76916  3596 15036  6641   727
    3    30   249 11661   348  1147     2
    4    42   135    88  4100    42     1
    5    18   152   466   157  5871     0
    6   387    41     3     4    10 11297
accuracy 0.83064
loss 2.69866

neota:
f1_score 0.77107
auroc 0.86542
auprc 0.80663
recall 0.78540
precision 0.78979
confusion_matrix
    0     1     2     3
    1 16774  1012   628
    2  4453  4320   146
    3    67     0  1985
accuracy 0.78755
loss 0.96336

poudre:
f1_score 0.78314
auroc 0.84423
auprc 0.66698
recall 0.77673
precision 0.82814
confusion_matrix
    0     1     2     3     4
    1   841  1508    61   596
    2   133 16276  1840  1908
    3     0     5   576     6
    4    51   659   443  7390
accuracy 0.73338
loss 2.24941

rawah:
f1_score 0.74667
auroc 0.86888
auprc 0.57203
recall 0.73331
precision 0.77532
confusion_matrix
     0      1      2      3      4
     1  76255  19185   1368   7847
     2  30043 105458   8145   1417
     3     20    143   2762      0
     4    375     45      4   4132
accuracy 0.82671
loss 1.73034

saving test results to cover/log/89/seed_22//140314/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 23...(16/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_23//140655/20230409_orphaned_cover_split_by_wilderness_model_23/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.991 valid 0.841 comanche:auroc: train 0.976 valid 0.935 comanche:auprc: train 0.849 valid 0.738 comanche:recall: train 0.991 valid 0.843 comanche:precision: train 0.991 valid 0.841 comanche:accuracy: train 0.990 valid 0.826 comanche:loss: train 0.037 valid 1.303 neota:f1_score: train 0.987 valid 0.918 neota:auroc: train 0.988 valid 0.955 neota:auprc: train 0.958 valid 0.876 neota:recall: train 0.987 valid 0.920 neota:precision: train 0.987 valid 0.917 neota:accuracy: train 0.980 valid 0.848 neota:loss: train 0.033 valid 0.565 poudre:f1_score: train 0.986 valid 0.898 poudre:auroc: train 0.966 valid 0.829 poudre:auprc: train 0.877 valid 0.636 poudre:recall: train 0.986 valid 0.899 poudre:precision: train 0.986 valid 0.900 poudre:accuracy: train 0.970 valid 0.751 poudre:loss: train 0.050 valid 0.687 rawah:f1_score: train 0.977 valid 0.823 rawah:auroc: train 0.987 valid 0.935 rawah:auprc: train 0.970 valid 0.854 rawah:recall: train 0.977 valid 0.824 rawah:precision: train 0.977 valid 0.823 rawah:accuracy: train 0.977 valid 0.843 rawah:loss: train 0.059 valid 1.295 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.72782
auroc 0.91561
auprc 0.61985
recall 0.71600
precision 0.76433
confusion_matrix
    0     1     2     3     4     5     6
    1 62502 16979   379  2168   523  4060
    2 20141 81536  3887 11310  6529   750
    3    45   200 11480   291  1411    10
    4    44   151    89  4035    86     3
    5    25   140   399   118  5978     4
    6   344    49    10     4     3 11332
accuracy 0.83504
loss 2.47740

neota:
f1_score 0.76282
auroc 0.85313
auprc 0.75718
recall 0.76913
precision 0.76966
confusion_matrix
    0     1     2     3
    1 15664  1852   898
    2  3755  4920   244
    3    34     1  2017
accuracy 0.79508
loss 774.50781

poudre:
f1_score 0.78731
auroc 0.89904
auprc 0.71944
recall 0.79308
precision 0.83149
confusion_matrix
    0     1     2     3     4
    1   694  1548    57   707
    2    49 16726  1116  2266
    3     0    10   563    14
    4    25   624   266  7628
accuracy 0.72817
loss 1.99006

rawah:
f1_score 0.75966
auroc 0.90161
auprc 0.66761
recall 0.74952
precision 0.78337
confusion_matrix
     0      1      2      3      4
     1  78702  18370    919   6664
     2  30126 107151   7211    575
     3     17     94   2814      0
     4    363     83      0   4110
accuracy 0.83871
loss 1.57367

saving test results to cover/log/89/seed_23//140655/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 24...(17/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_24//141035/20230409_orphaned_cover_split_by_wilderness_model_24/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.999 valid 0.848 comanche:auroc: train 0.983 valid 0.938 comanche:auprc: train 0.871 valid 0.732 comanche:recall: train 0.999 valid 0.852 comanche:precision: train 0.999 valid 0.847 comanche:accuracy: train 0.999 valid 0.835 comanche:loss: train 0.003 valid 1.485 neota:f1_score: train 0.979 valid 0.869 neota:auroc: train 0.671 valid 0.651 neota:auprc: train 0.549 valid 0.546 neota:recall: train 0.979 valid 0.867 neota:precision: train 0.979 valid 0.872 neota:accuracy: train 0.968 valid 0.784 neota:loss: train 0.078 valid 0.417 poudre:f1_score: train 0.983 valid 0.911 poudre:auroc: train 0.936 valid 0.861 poudre:auprc: train 0.820 valid 0.640 poudre:recall: train 0.983 valid 0.913 poudre:precision: train 0.983 valid 0.914 poudre:accuracy: train 0.969 valid 0.720 poudre:loss: train 0.045 valid 0.514 rawah:f1_score: train 0.969 valid 0.809 rawah:auroc: train 0.969 valid 0.925 rawah:auprc: train 0.909 valid 0.842 rawah:recall: train 0.969 valid 0.811 rawah:precision: train 0.969 valid 0.810 rawah:accuracy: train 0.971 valid 0.836 rawah:loss: train 0.091 valid 0.953 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.72635
auroc 0.91919
auprc 0.61956
recall 0.71361
precision 0.77011
confusion_matrix
    0     1     2     3     4     5     6
    1 64617 14406   286  2603   448  4251
    2 21405 78416  4135 12402  6704  1091
    3    29   234 11776   319  1073     6
    4    35   155    99  4080    38     1
    5    15   120   433   120  5973     3
    6   291    27     5     5     3 11411
accuracy 0.84129
loss 2.98756

neota:
f1_score 0.67185
auroc 0.65847
auprc 0.49626
recall 0.67922
precision 0.67733
confusion_matrix
    0     1     2     3
    1 14051  3049  1314
    2  4698  3910   311
    3    47     7  1998
accuracy 0.72504
loss 9.70477

poudre:
f1_score 0.80217
auroc 0.87822
auprc 0.72142
recall 0.80553
precision 0.84272
confusion_matrix
    0     1     2     3     4
    1   796  1710    79   421
    2    28 17305  1165  1659
    3     0    14   566     7
    4    20   821   356  7346
accuracy 0.73686
loss 1.80511

rawah:
f1_score 0.75028
auroc 0.87493
auprc 0.59300
recall 0.73290
precision 0.78426
confusion_matrix
     0      1      2      3      4
     1  74108  18957   1304  10286
     2  26673 107318   9474   1598
     3     22     94   2809      0
     4    245     45      0   4266
accuracy 0.83615
loss 1.52519

saving test results to cover/log/89/seed_24//141035/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 25...(18/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_25//141414/20230409_orphaned_cover_split_by_wilderness_model_25/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.999 valid 0.851 comanche:auroc: train 0.984 valid 0.937 comanche:auprc: train 0.884 valid 0.746 comanche:recall: train 0.999 valid 0.854 comanche:precision: train 0.999 valid 0.850 comanche:accuracy: train 0.999 valid 0.838 comanche:loss: train 0.002 valid 1.488 neota:f1_score: train 0.997 valid 0.908 neota:auroc: train 0.998 valid 0.942 neota:auprc: train 0.997 valid 0.870 neota:recall: train 0.997 valid 0.912 neota:precision: train 0.997 valid 0.908 neota:accuracy: train 0.994 valid 0.840 neota:loss: train 0.005 valid 1.306 poudre:f1_score: train 0.982 valid 0.900 poudre:auroc: train 0.941 valid 0.877 poudre:auprc: train 0.743 valid 0.606 poudre:recall: train 0.982 valid 0.901 poudre:precision: train 0.982 valid 0.900 poudre:accuracy: train 0.933 valid 0.750 poudre:loss: train 0.055 valid 0.556 rawah:f1_score: train 0.985 valid 0.833 rawah:auroc: train 0.986 valid 0.936 rawah:auprc: train 0.964 valid 0.857 rawah:recall: train 0.985 valid 0.835 rawah:precision: train 0.985 valid 0.834 rawah:accuracy: train 0.986 valid 0.858 rawah:loss: train 0.053 valid 1.251 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.72267
auroc 0.91953
auprc 0.61957
recall 0.71121
precision 0.76723
confusion_matrix
    0     1     2     3     4     5     6
    1 64800 13683   169  2295   408  5256
    2 23243 77513  3837 11905  6613  1042
    3    35   220 11817   320  1019    26
    4    32   169    91  4071    45     0
    5    28   148   397   125  5964     2
    6   197    23     0     7     0 11515
accuracy 0.84185
loss 3.10499

neota:
f1_score 0.77115
auroc 0.90476
auprc 0.83996
recall 0.77563
precision 0.77950
confusion_matrix
    0     1     2     3
    1 15618  1831   965
    2  3438  5133   348
    3    10     1  2041
accuracy 0.80610
loss 2.45254

poudre:
f1_score 0.80128
auroc 0.86594
auprc 0.68130
recall 0.80141
precision 0.83679
confusion_matrix
    0     1     2     3     4
    1   852  1634   114   406
    2    55 17288  1264  1550
    3     0     8   571     8
    4    83   884   407  7169
accuracy 0.73825
loss 1.51837

rawah:
f1_score 0.77461
auroc 0.89683
auprc 0.70134
recall 0.76296
precision 0.79844
confusion_matrix
     0      1      2      3      4
     1  76434  19277   1325   7619
     2  24121 112686   7398    858
     3     17    119   2789      0
     4    187     43      3   4323
accuracy 0.85238
loss 1.59370

saving test results to cover/log/89/seed_25//141414/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 26...(19/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_26//141751/20230409_orphaned_cover_split_by_wilderness_model_26/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.932 valid 0.828 comanche:auroc: train 0.953 valid 0.929 comanche:auprc: train 0.772 valid 0.716 comanche:recall: train 0.932 valid 0.830 comanche:precision: train 0.931 valid 0.828 comanche:accuracy: train 0.926 valid 0.815 comanche:loss: train 0.171 valid 0.652 neota:f1_score: train 0.997 valid 0.870 neota:auroc: train 0.979 valid 0.923 neota:auprc: train 0.881 valid 0.740 neota:recall: train 0.997 valid 0.876 neota:precision: train 0.997 valid 0.867 neota:accuracy: train 0.994 valid 0.776 neota:loss: train 0.004 valid 2.145 poudre:f1_score: train 0.990 valid 0.903 poudre:auroc: train 0.993 valid 0.909 poudre:auprc: train 0.985 valid 0.727 poudre:recall: train 0.990 valid 0.905 poudre:precision: train 0.990 valid 0.902 poudre:accuracy: train 0.974 valid 0.673 poudre:loss: train 0.037 valid 0.563 rawah:f1_score: train 0.986 valid 0.826 rawah:auroc: train 0.975 valid 0.916 rawah:auprc: train 0.932 valid 0.829 rawah:recall: train 0.986 valid 0.828 rawah:precision: train 0.986 valid 0.826 rawah:accuracy: train 0.987 valid 0.851 rawah:loss: train 0.047 valid 1.136 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.72368
auroc 0.88647
auprc 0.55625
recall 0.70769
precision 0.76555
confusion_matrix
    0     1     2     3     4     5     6
    1 60749 18694   314  3042   325  3487
    2 16912 81066  4324 14086  7037   728
    3     3   139 11791   244  1258     2
    4    25   144   125  4055    59     0
    5    10   128   413   129  5984     0
    6   537    34     0     6     0 11165
accuracy 0.83343
loss 1.15099

neota:
f1_score 0.77667
auroc 0.84563
auprc 0.78261
recall 0.77900
precision 0.78729
confusion_matrix
    0     1     2     3
    1 15464  1723  1227
    2  3264  5392   263
    3    15     2  2035
accuracy 0.81202
loss 3.54488

poudre:
f1_score 0.78569
auroc 0.87378
auprc 0.62334
recall 0.79017
precision 0.82992
confusion_matrix
    0     1     2     3     4
    1   658  1692   138   518
    2    49 16821  1225  2062
    3     0    13   562    12
    4    34   693   340  7476
accuracy 0.72148
loss 2.25135

rawah:
f1_score 0.76535
auroc 0.88596
auprc 0.65622
recall 0.75580
precision 0.78889
confusion_matrix
     0      1      2      3      4
     1  80188  17453   1006   6008
     2  30123 107241   6613   1086
     3     34    122   2769      0
     4    326     36      0   4194
accuracy 0.84317
loss 1.61787

saving test results to cover/log/89/seed_26//141751/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 27...(20/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_27//142131/20230409_orphaned_cover_split_by_wilderness_model_27/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.983 valid 0.826 comanche:auroc: train 0.990 valid 0.941 comanche:auprc: train 0.924 valid 0.779 comanche:recall: train 0.983 valid 0.827 comanche:precision: train 0.983 valid 0.826 comanche:accuracy: train 0.982 valid 0.811 comanche:loss: train 0.047 valid 1.158 neota:f1_score: train 0.916 valid 0.830 neota:auroc: train 0.977 valid 0.925 neota:auprc: train 0.907 valid 0.731 neota:recall: train 0.916 valid 0.858 neota:precision: train 0.917 valid 0.854 neota:accuracy: train 0.879 valid 0.687 neota:loss: train 0.228 valid 0.553 poudre:f1_score: train 0.991 valid 0.911 poudre:auroc: train 0.996 valid 0.974 poudre:auprc: train 0.991 valid 0.840 poudre:recall: train 0.991 valid 0.912 poudre:precision: train 0.991 valid 0.910 poudre:accuracy: train 0.993 valid 0.759 poudre:loss: train 0.026 valid 0.475 rawah:f1_score: train 0.994 valid 0.850 rawah:auroc: train 0.987 valid 0.943 rawah:auprc: train 0.963 valid 0.863 rawah:recall: train 0.994 valid 0.851 rawah:precision: train 0.994 valid 0.849 rawah:accuracy: train 0.995 valid 0.872 rawah:loss: train 0.021 valid 1.047 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.72749
auroc 0.91435
auprc 0.64834
recall 0.71610
precision 0.76179
confusion_matrix
    0     1     2     3     4     5     6
    1 63112 17592   157  2196   593  2961
    2 20923 81136  3356 10666  7175   897
    3    43   292 11614   352  1124    12
    4    57   227    79  3984    57     4
    5    62   154   442   157  5847     2
    6   491    50     0     4     3 11194
accuracy 0.83018
loss 1.95194

neota:
f1_score 0.73676
auroc 0.83450
auprc 0.80062
recall 0.76699
precision 0.77948
confusion_matrix
    0     1     2     3
    1 17401   648   365
    2  5585  3148   186
    3    63     0  1989
accuracy 0.75575
loss 1.27658

poudre:
f1_score 0.79977
auroc 0.89924
auprc 0.71116
recall 0.79980
precision 0.84448
confusion_matrix
    0     1     2     3     4
    1   842  1389   125   650
    2    36 16881  1326  1914
    3     0     8   570     9
    4    23   679   306  7535
accuracy 0.74266
loss 1.92462

rawah:
f1_score 0.77244
auroc 0.88849
auprc 0.64236
recall 0.76084
precision 0.79674
confusion_matrix
     0      1      2      3      4
     1  79579  18246   1544   5286
     2  26462 109102   8737    762
     3      8     71   2846      0
     4    316     81      0   4159
accuracy 0.84959
loss 1.68286

saving test results to cover/log/89/seed_27//142131/test_results.txt
Done orphaned testing.
Loading orphaned trainer for seed 28...(21/21)
Label encodings are (we are using these):
comanche label encodings [1, 2, 3, 5, 6, 7] => [0, 1, 2, 3, 4, 5]
neota label encodings [1, 2, 7] => [0, 1, 2]
poudre label encodings [2, 3, 4, 6] => [0, 1, 2, 3]
rawah label encodings [1, 2, 5, 7] => [0, 1, 2, 3]
loading model cover.models.model
--personalize set to False; loading FullyFederatedCovertypeModel ...
name:cover_comanche,input_shape:(11,):output_shape(6,)
name:cover_neota,input_shape:(11,):output_shape(3,)
name:cover_poudre,input_shape:(11,):output_shape(4,)
name:cover_rawah,input_shape:(11,):output_shape(4,)
Loaded SummaryWriter at cover/log/89/seed_28//142511/20230409_orphaned_cover_split_by_wilderness_model_28/
Starting orphaned training...
                                                                                                                                                          
Final epoch metrics:                                                                                                                                      
Epoch 105: comanche:f1_score: train 0.933 valid 0.806 comanche:auroc: train 0.945 valid 0.914 comanche:auprc: train 0.745 valid 0.686 comanche:recall: train 0.933 valid 0.812 comanche:precision: train 0.933 valid 0.809 comanche:accuracy: train 0.928 valid 0.794 comanche:loss: train 0.224 valid 1.015 neota:f1_score: train 0.960 valid 0.861 neota:auroc: train 0.912 valid 0.904 neota:auprc: train 0.736 valid 0.707 neota:recall: train 0.961 valid 0.867 neota:precision: train 0.960 valid 0.861 neota:accuracy: train 0.939 valid 0.784 neota:loss: train 0.132 valid 1.313 poudre:f1_score: train 0.988 valid 0.908 poudre:auroc: train 0.955 valid 0.905 poudre:auprc: train 0.801 valid 0.672 poudre:recall: train 0.988 valid 0.909 poudre:precision: train 0.988 valid 0.909 poudre:accuracy: train 0.901 valid 0.757 poudre:loss: train 0.047 valid 0.518 rawah:f1_score: train 0.969 valid 0.823 rawah:auroc: train 0.976 valid 0.904 rawah:auprc: train 0.946 valid 0.812 rawah:recall: train 0.969 valid 0.825 rawah:precision: train 0.969 valid 0.822 rawah:accuracy: train 0.971 valid 0.853 rawah:loss: train 0.109 valid 0.923 

Done orphaned training. Starting testing....
Test results:

comanche:
f1_score 0.66099
auroc 0.88813
auprc 0.57330
recall 0.65636
precision 0.73937
confusion_matrix
    0     1     2     3     4     5     6
    1 69421 10101   227  2307   454  4101
    2 35262 59901  5670 14242  7921  1157
    3    22   161 11623   341  1281     9
    4    91   131    85  4030    66     5
    5    13   139   480   107  5910    15
    6   450    42     0     3     1 11246
accuracy 0.81798
loss 1.80992

neota:
f1_score 0.74628
auroc 0.76198
auprc 0.61648
recall 0.74797
precision 0.75260
confusion_matrix
    0     1     2     3
    1 14609  2639  1166
    2  3428  5324   167
    3     4     2  2046
accuracy 0.79579
loss 1.61002

poudre:
f1_score 0.79717
auroc 0.86051
auprc 0.65423
recall 0.80020
precision 0.83567
confusion_matrix
    0     1     2     3     4
    1   642  1421   239   704
    2    87 17173  1308  1589
    3     0    13   565     9
    4    81   690   311  7461
accuracy 0.72535
loss 1.74835

rawah:
f1_score 0.74783
auroc 0.88083
auprc 0.69280
recall 0.73321
precision 0.78273
confusion_matrix
     0      1      2      3      4
     1  79747  16632   1563   6713
     2  31834 101744  10094   1391
     3      3     89   2833      0
     4    277     21      0   4258
accuracy 0.84163
loss 1.27105

saving test results to cover/log/89/seed_28//142511/test_results.txt
Done orphaned testing.